{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Hyperbench Documentation","text":"<p>Welcome to Hyperbench - A comprehensive benchmarking framework for hypergraph neural networks.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>User Guide</li> <li>API Reference</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Hyperbench provides tools for working with hypergraph data in machine learning contexts. It supports loading datasets in HIF format, transforming them into PyTorch tensors, and preparing them for training hypergraph neural networks.</p>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"contributing/","title":"How to contribute","text":"<p>The project main language is English.</p>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>Run the following command to install the pre-commit hook:</p> <pre><code>make setup\n\npre-commit install --config .github/hooks/.pre-commit-config.yaml --hook-type pre-commit --install-hooks --overwrite\n</code></pre> <p>This will ensure that your code adheres to the project's coding standards before each commit.</p>"},{"location":"contributing/#commit-message-style","title":"Commit message style","text":"<p>Commit messages should follow the conventional commit specification.</p> <p>The allowed structural elements are: - <code>feat</code> for new features. - <code>fix</code> for bug fixes. - <code>chore</code> for changes to the build process or auxiliary tools and libraries such as documentation generation. - <code>refactor</code> for code changes that neither fix a bug nor add a feature. - <code>docs</code> for any documentation/README changes.</p> <p>Commit messages should be structured in a way that can be read as if they were completing the sentence \"If applied, this commit will...\". For example:</p> <p>feat: add new authentication method to API</p> <p>Reads as \"If applied, this commit will add new authentication method to API\".</p>"},{"location":"contributing/#branch-naming","title":"Branch naming","text":"<p>Branch names should be descriptive and use hyphens to separate words. They should also follow the same structure as commit messages, using the allowed structural elements. For example: - <code>feat/add-user-authentication</code> - <code>fix/issue-with-database-connection</code> - <code>chore/update-dependencies</code> - <code>refactor/improve-code-structure</code> - <code>docs/update-contributing-guidelines</code></p>"},{"location":"ref/","title":"References","text":"<p>This document provides references to various resources and materials related to the topics covered in this documentation.</p>"},{"location":"ref/#hypergraphdata-type","title":"HyperGraphData type","text":"<p>PyTorch Geometric HyperGraphData on GitHub.</p>"},{"location":"ref/#hyperbench-v0","title":"HyperBench v0","text":"<p>GitHub Repository.</p>"},{"location":"api/reference/","title":"API Reference","text":"<p>Complete API documentation for all Hyperbench modules.</p>"},{"location":"api/reference/#data-module","title":"Data Module","text":""},{"location":"api/reference/#hyperbench.data.dataset","title":"<code>hyperbench.data.dataset</code>","text":""},{"location":"api/reference/#hyperbench.data.dataset.DatasetNames","title":"<code>DatasetNames</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of available datasets.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>class DatasetNames(Enum):\n    \"\"\"\n    Enumeration of available datasets.\n    \"\"\"\n\n    ALGEBRA = \"algebra\"\n    AMAZON = \"amazon\"\n    CONTACT_HIGH_SCHOOL = \"contact-high-school\"\n    CONTACT_PRIMARY_SCHOOL = \"contact-primary-school\"\n    CORA = \"cora\"\n    COURSERA = \"coursera\"\n    DBLP = \"dblp\"\n    EMAIL_ENRON = \"email-Enron\"\n    EMAIL_W3C = \"email-W3C\"\n    GEOMETRY = \"geometry\"\n    GOT = \"got\"\n    IMBD = \"imdb\"\n    MUSIC_BLUES_REVIEWS = \"music-blues-reviews\"\n    NBA = \"nba\"\n    NDC_CLASSES = \"NDC-classes\"\n    NDC_SUBSTANCES = \"NDC-substances\"\n    RESTAURANT_REVIEWS = \"restaurant-reviews\"\n    THREADS_ASK_UBUNTU = \"threads-ask-ubuntu\"\n    THREADS_MATH_SX = \"threads-math-sx\"\n    TWITTER = \"twitter\"\n    VEGAS_BARS_REVIEWS = \"vegas-bars-reviews\"\n    PATENT = \"patent\"\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.HIFConverter","title":"<code>HIFConverter</code>","text":"<p>Docstring for HIFConverter A utility class to load hypergraphs from HIF format.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>class HIFConverter:\n    \"\"\"\n    Docstring for HIFConverter\n    A utility class to load hypergraphs from HIF format.\n    \"\"\"\n\n    @staticmethod\n    def load_from_hif(dataset_name: Optional[str], save_on_disk: bool = False) -&gt; HIFHypergraph:\n        if dataset_name is None:\n            raise ValueError(f\"Dataset name (provided: {dataset_name}) must be provided.\")\n        if dataset_name not in DatasetNames.__members__:\n            raise ValueError(f\"Dataset '{dataset_name}' not found.\")\n\n        dataset_name = DatasetNames[dataset_name].value\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        zst_filename = os.path.join(current_dir, \"datasets\", f\"{dataset_name}.json.zst\")\n\n        if not os.path.exists(zst_filename):\n            github_dataset_repo = f\"https://github.com/hypernetwork-research-group/datasets/blob/main/{dataset_name}.json.zst?raw=true\"\n\n            response = requests.get(github_dataset_repo)\n            if response.status_code != 200:\n                raise ValueError(\n                    f\"Failed to download dataset '{dataset_name}' from GitHub. Status code: {response.status_code}\"\n                )\n\n            if save_on_disk:\n                os.makedirs(os.path.join(current_dir, \"datasets\"), exist_ok=True)\n                with open(zst_filename, \"wb\") as f:\n                    f.write(response.content)\n            else:\n                # Create temporary file for downloaded zst content\n                with tempfile.NamedTemporaryFile(\n                    mode=\"wb\", suffix=\".json.zst\", delete=False\n                ) as tmp_zst_file:\n                    tmp_zst_file.write(response.content)\n                    zst_filename = tmp_zst_file.name\n\n        # Decompress the downloaded zst file\n        dctx = zstd.ZstdDecompressor()\n        with (\n            open(zst_filename, \"rb\") as input_f,\n            tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".json\", delete=False) as tmp_file,\n        ):\n            dctx.copy_stream(input_f, tmp_file)\n            output = tmp_file.name\n\n        with open(output, \"r\") as f:\n            hiftext = json.load(f)\n        if not validate_hif_json(output):\n            raise ValueError(f\"Dataset '{dataset_name}' is not HIF-compliant.\")\n\n        hypergraph = HIFHypergraph.from_hif(hiftext)\n        return hypergraph\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>class Dataset(TorchDataset):\n    DATASET_NAME = None\n\n    def __init__(self) -&gt; None:\n        self.hypergraph: HIFHypergraph = self.download()\n        self.hdata: HData = self.process()\n\n    def __len__(self) -&gt; int:\n        return len(self.hypergraph.nodes)\n\n    def __getitem__(self, index: int | List[int]) -&gt; HData:\n        sampled_node_ids_list = self.__get_node_ids_to_sample(index)\n        self.__validate_node_ids(sampled_node_ids_list)\n\n        sampled_hyperedge_index, sampled_node_ids, sampled_hyperedge_ids = (\n            self.__sample_hyperedge_index(sampled_node_ids_list)\n        )\n\n        new_hyperedge_index = self.__new_hyperedge_index(\n            sampled_hyperedge_index, sampled_node_ids, sampled_hyperedge_ids\n        )\n\n        new_x = self.hdata.x[sampled_node_ids]\n\n        new_edge_attr = None\n        if self.hdata.edge_attr is not None and len(sampled_hyperedge_ids) &gt; 0:\n            new_edge_attr = self.hdata.edge_attr[sampled_hyperedge_ids]\n\n        return HData(\n            x=new_x,\n            edge_index=new_hyperedge_index,\n            edge_attr=new_edge_attr,\n            num_nodes=len(sampled_node_ids),\n            num_edges=len(sampled_hyperedge_ids),\n        )\n\n    def download(self) -&gt; HIFHypergraph:\n        \"\"\"\n        Load the hypergraph from HIF format using HIFConverter class.\n        \"\"\"\n        if hasattr(self, \"hypergraph\") and self.hypergraph is not None:\n            return self.hypergraph\n        hypergraph = HIFConverter.load_from_hif(self.DATASET_NAME)\n        return hypergraph\n\n    def process(self) -&gt; HData:\n        \"\"\"\n        Process the loaded hypergraph into HData format, mapping HIF structure to tensors.\n\n        Returns:\n            HData: Processed hypergraph data.\n        \"\"\"\n        num_nodes = len(self.hypergraph.nodes)\n        num_edges = len(self.hypergraph.edges)\n\n        # x: shape [num_nodes, num_node_features]\n        # collect all attribute keys to have tensors of same size\n        node_attr_keys = self.__collect_attr_keys(\n            [node.get(\"attrs\", {}) for node in self.hypergraph.nodes]\n        )\n\n        if node_attr_keys:\n            x = torch.stack(\n                [\n                    self.transform_node_attrs(node.get(\"attrs\", {}), attr_keys=node_attr_keys)\n                    for node in self.hypergraph.nodes\n                ]\n            )\n        else:\n            # Fallback to ones if no node features, 1 is better as it can help during\n            # training (e.g., avoid zero multiplication), especially in first epochs\n            x = torch.ones((num_nodes, 1), dtype=torch.float)\n\n        # remap node and edge IDs to 0-based contiguous IDs\n        # Use dict comprehension for faster lookups\n        node_set = {}\n        edge_set = {}\n        node_ids = []\n        edge_ids = []\n\n        for inc in self.hypergraph.incidences:\n            node = inc.get(\"node\", 0)\n            edge = inc.get(\"edge\", 0)\n\n            if node not in node_set:\n                node_set[node] = len(node_set)\n            if edge not in edge_set:\n                edge_set[edge] = len(edge_set)\n\n            node_ids.append(node_set[node])\n            edge_ids.append(edge_set[edge])\n\n        if len(node_ids) &lt; 1:\n            raise ValueError(\"Hypergraph has no incidences.\")\n\n        # hyperedge_index: shape [2, E] where E is number of incidences\n        hyperedge_index = torch.tensor([node_ids, edge_ids], dtype=torch.long)\n\n        # hyperedge-attr: shape [num_hyperedges, num_hyperedge_attributes]\n        hyperedge_attr = None\n        if self.hypergraph.edges and any(\"attrs\" in edge for edge in self.hypergraph.edges):\n            # collect all attribute keys to have tensors of same size\n            hyperedge_attr_keys = self.__collect_attr_keys(\n                [edge.get(\"attrs\", {}) for edge in self.hypergraph.edges]\n            )\n\n            hyperedge_attr = torch.stack(\n                [\n                    self.transform_hyperedge_attrs(\n                        edge.get(\"attrs\", {}), attr_keys=hyperedge_attr_keys\n                    )\n                    for edge in self.hypergraph.edges\n                ]\n            )\n\n            # Flatten to 1D if only one attribute (PyTorch Geometric standard)\n            # if edge_attr.shape[1] == 1:\n            #     hyperedge_attr = hyperedge_attr.squeeze(1)\n\n        return HData(x, hyperedge_index, hyperedge_attr, num_nodes, num_edges)\n\n    def transform_node_attrs(\n        self,\n        attrs: Dict[str, Any],\n        attr_keys: Optional[List[str]] = None,\n    ) -&gt; Tensor:\n        return self.transform_attrs(attrs, attr_keys)\n\n    def transform_hyperedge_attrs(\n        self,\n        attrs: Dict[str, Any],\n        attr_keys: Optional[List[str]] = None,\n    ) -&gt; Tensor:\n        return self.transform_attrs(attrs, attr_keys)\n\n    def transform_attrs(\n        self,\n        attrs: Dict[str, Any],\n        attr_keys: Optional[List[str]] = None,\n    ) -&gt; Tensor:\n        r\"\"\"\n        Extract and encode numeric attributes to tensor.\n        Non-numeric attributes are discarded. Missing attributes are filled with ``0.0``.\n\n        Args:\n            attrs: Dictionary of attributes\n            attr_keys: Optional list of attribute keys to encode. If provided, ensures consistent ordering and fill missing with ``0.0``.\n\n        Returns:\n            Tensor of numeric attribute values\n        \"\"\"\n        numeric_attrs = {\n            key: value\n            for key, value in attrs.items()\n            if isinstance(value, (int, float)) and not isinstance(value, bool)\n        }\n\n        if attr_keys is not None:\n            values = [float(numeric_attrs.get(key, 0.0)) for key in attr_keys]\n            return torch.tensor(values, dtype=torch.float)\n\n        if not numeric_attrs:\n            return torch.tensor([], dtype=torch.float)\n\n        values = [float(value) for value in numeric_attrs.values()]\n        return torch.tensor(values, dtype=torch.float)\n\n    def __collect_attr_keys(self, attr_keys: List[Dict[str, Any]]) -&gt; List[str]:\n        \"\"\"\n        Collect unique numeric attribute keys from a list of attribute dictionaries.\n\n        Args:\n            attrs_list: List of attribute dictionaries.\n\n        Returns:\n            List of unique numeric attribute keys.\n        \"\"\"\n        unique_keys = []\n        for attrs in attr_keys:\n            for key, value in attrs.items():\n                if key not in unique_keys and isinstance(value, (int, float)):\n                    unique_keys.append(key)\n\n        return unique_keys\n\n    def __get_node_ids_to_sample(self, id: int | List[int]) -&gt; List[int]:\n        \"\"\"\n        Get a list of node IDs to sample based on the provided index.\n\n        Args:\n            id: An integer or a list of integers representing node IDs to sample.\n\n        Returns:\n            List of node IDs to sample.\n\n        Raises:\n            ValueError: If the provided index is invalid (e.g., empty list or list length exceeds number of nodes).\n        \"\"\"\n        if isinstance(id, list):\n            if len(id) &lt; 1:\n                raise ValueError(\"Index list cannot be empty.\")\n            elif len(id) &gt; self.__len__():\n                raise ValueError(\n                    \"Index list length cannot exceed number of nodes in the hypergraph.\"\n                )\n            return list(set(id))\n\n        return [id]\n\n    def __validate_node_ids(self, node_ids: List[int]) -&gt; None:\n        \"\"\"\n        Validate that node IDs are within bounds of the hypergraph.\n\n        Args:\n            node_ids: List of node IDs to validate.\n\n        Raises:\n            IndexError: If any node ID is out of bounds.\n        \"\"\"\n        for id in node_ids:\n            if id &lt; 0 or id &gt;= self.__len__():\n                raise IndexError(f\"Node ID {id} is out of bounds (0, {self.__len__() - 1}).\")\n\n    def __sample_hyperedge_index(\n        self,\n        sampled_node_ids_list: List[int],\n    ) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        hyperedge_index = self.hdata.edge_index\n        node_ids = hyperedge_index[0]\n        hyperedge_ids = hyperedge_index[1]\n\n        sampled_node_ids = torch.tensor(sampled_node_ids_list, device=node_ids.device)\n\n        # Find incidences where the node is in our sampled node set\n        # Example: hyperedge_index[0] = [0, 0, 1, 2, 3, 4], sampled_node_ids = [0, 3]\n        #          -&gt; node_incidence_mask = [True, True, False, False, True, False]\n        node_incidence_mask = torch.isin(node_ids, sampled_node_ids)\n\n        # Get unique hyperedges that have at least one sampled node\n        # Example: hyperedge_index[1] = [0, 0, 0, 1, 2, 2], node_incidence_mask = [True, True, False, False, True, False]\n        #          -&gt; sampled_hyperedge_ids = [0, 2] as they connect to sampled nodes\n        sampled_hyperedge_ids = hyperedge_ids[node_incidence_mask].unique()\n\n        # Find all incidences for sampled nodes belonging to relevant hyperedges\n        # Example: hyperedge_index[1] = [0, 0, 0, 1, 2, 2], sampled_hyperedge_ids = [0, 2]\n        #          -&gt; hyperedge_incidence_mask = [True, True, True, False, True, True]\n        hyperedge_incidence_mask = torch.isin(hyperedge_ids, sampled_hyperedge_ids)\n\n        # Incidence is kept if node is sampled AND hyperedge is relevant\n        incidence_mask = node_incidence_mask &amp; hyperedge_incidence_mask\n\n        # Keep only the incidences that match our mask\n        # Example: hyperedge_index = [[0, 0, 1, 2, 3, 4],\n        #                              [0, 0, 0, 1, 2, 2]],\n        #          incidence_mask = [True, True, False, False, True, False]\n        #          -&gt; sampled_hyperedge_index = [[0, 0, 3],\n        #                                        [0, 0, 2]]\n        sampled_hyperedge_index = hyperedge_index[:, incidence_mask]\n        return sampled_hyperedge_index, sampled_node_ids, sampled_hyperedge_ids\n\n    def __new_hyperedge_index(\n        self,\n        sampled_hyperedge_index: Tensor,\n        sampled_node_ids: Tensor,\n        sampled_hyperedge_ids: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"\n        Create new hyperedge_index with 0-based node and hyperedge IDs.\n\n        Args:\n            sampled_hyperedge_index: Original hyperedge_index tensor with sampled incidences.\n            sampled_node_ids: List of sampled original node IDs.\n            sampled_hyperedge_ids: List of sampled original hyperedge IDs.\n\n        Returns:\n            New hyperedge_index tensor with 0-based node and edge IDs.\n        \"\"\"\n        # Example: sampled_edge_index = [[1, 1, 3],\n        #                                [0, 2, 2]]\n        #          sampled_node_ids = [1, 3],\n        #          sampled_edge_ids = [0, 2]\n        #          -&gt; new_node_ids = [0, 0, 1], new_edge_ids = [0, 1, 1]\n        new_node_ids = self.__to_0based_ids(\n            sampled_hyperedge_index[0], sampled_node_ids, self.hdata.num_nodes\n        )\n        new_hyperedge_ids = self.__to_0based_ids(\n            sampled_hyperedge_index[1], sampled_hyperedge_ids, self.hdata.num_edges\n        )\n\n        # Example: new_node_ids = [0, 1], new_hyperedge_ids = [0, 1]\n        #          -&gt; new_hyperedge_index = [[0, 1],\n        #                                    [0, 1]]\n        new_hyperedge_index = torch.stack([new_node_ids, new_hyperedge_ids], dim=0)\n        return new_hyperedge_index\n\n    def __to_0based_ids(\n        self,\n        original_ids: Tensor,\n        ids_to_keep: Tensor,\n        n: int,\n    ) -&gt; Tensor:\n        \"\"\"\n        Map original IDs to 0-based ids.\n\n        Example:\n            original_ids: [1, 3, 3, 7]\n            ids_to_keep: [3, 7]\n            n = 8                            # total number of elements (nodes or edges) in the original hypergraph\n            Returned 0-based IDs: [0, 0, 1]  # the size is sum of occurrences of ids_to_keep in original_ids\n\n        Args:\n            original_ids: Tensor of original IDs.\n            n: Total number of original IDs.\n            ids_to_keep: List of selected original IDs to be mapped to 0-based.\n\n        Returns:\n            Tensor of 0-based ids.\n        \"\"\"\n        device = original_ids.device\n\n        id_to_0based_id = torch.zeros(n, dtype=torch.long, device=device)\n        n_ids_to_keep = len(ids_to_keep)\n        id_to_0based_id[ids_to_keep] = torch.arange(n_ids_to_keep, device=device)\n        return id_to_0based_id[original_ids]\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.download","title":"<code>download()</code>","text":"<p>Load the hypergraph from HIF format using HIFConverter class.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def download(self) -&gt; HIFHypergraph:\n    \"\"\"\n    Load the hypergraph from HIF format using HIFConverter class.\n    \"\"\"\n    if hasattr(self, \"hypergraph\") and self.hypergraph is not None:\n        return self.hypergraph\n    hypergraph = HIFConverter.load_from_hif(self.DATASET_NAME)\n    return hypergraph\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.process","title":"<code>process()</code>","text":"<p>Process the loaded hypergraph into HData format, mapping HIF structure to tensors.</p> <p>Returns:     HData: Processed hypergraph data.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def process(self) -&gt; HData:\n    \"\"\"\n    Process the loaded hypergraph into HData format, mapping HIF structure to tensors.\n\n    Returns:\n        HData: Processed hypergraph data.\n    \"\"\"\n    num_nodes = len(self.hypergraph.nodes)\n    num_edges = len(self.hypergraph.edges)\n\n    # x: shape [num_nodes, num_node_features]\n    # collect all attribute keys to have tensors of same size\n    node_attr_keys = self.__collect_attr_keys(\n        [node.get(\"attrs\", {}) for node in self.hypergraph.nodes]\n    )\n\n    if node_attr_keys:\n        x = torch.stack(\n            [\n                self.transform_node_attrs(node.get(\"attrs\", {}), attr_keys=node_attr_keys)\n                for node in self.hypergraph.nodes\n            ]\n        )\n    else:\n        # Fallback to ones if no node features, 1 is better as it can help during\n        # training (e.g., avoid zero multiplication), especially in first epochs\n        x = torch.ones((num_nodes, 1), dtype=torch.float)\n\n    # remap node and edge IDs to 0-based contiguous IDs\n    # Use dict comprehension for faster lookups\n    node_set = {}\n    edge_set = {}\n    node_ids = []\n    edge_ids = []\n\n    for inc in self.hypergraph.incidences:\n        node = inc.get(\"node\", 0)\n        edge = inc.get(\"edge\", 0)\n\n        if node not in node_set:\n            node_set[node] = len(node_set)\n        if edge not in edge_set:\n            edge_set[edge] = len(edge_set)\n\n        node_ids.append(node_set[node])\n        edge_ids.append(edge_set[edge])\n\n    if len(node_ids) &lt; 1:\n        raise ValueError(\"Hypergraph has no incidences.\")\n\n    # hyperedge_index: shape [2, E] where E is number of incidences\n    hyperedge_index = torch.tensor([node_ids, edge_ids], dtype=torch.long)\n\n    # hyperedge-attr: shape [num_hyperedges, num_hyperedge_attributes]\n    hyperedge_attr = None\n    if self.hypergraph.edges and any(\"attrs\" in edge for edge in self.hypergraph.edges):\n        # collect all attribute keys to have tensors of same size\n        hyperedge_attr_keys = self.__collect_attr_keys(\n            [edge.get(\"attrs\", {}) for edge in self.hypergraph.edges]\n        )\n\n        hyperedge_attr = torch.stack(\n            [\n                self.transform_hyperedge_attrs(\n                    edge.get(\"attrs\", {}), attr_keys=hyperedge_attr_keys\n                )\n                for edge in self.hypergraph.edges\n            ]\n        )\n\n        # Flatten to 1D if only one attribute (PyTorch Geometric standard)\n        # if edge_attr.shape[1] == 1:\n        #     hyperedge_attr = hyperedge_attr.squeeze(1)\n\n    return HData(x, hyperedge_index, hyperedge_attr, num_nodes, num_edges)\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.transform_attrs","title":"<code>transform_attrs(attrs, attr_keys=None)</code>","text":"<p>Extract and encode numeric attributes to tensor. Non-numeric attributes are discarded. Missing attributes are filled with <code>0.0</code>.</p> <p>Args:     attrs: Dictionary of attributes     attr_keys: Optional list of attribute keys to encode. If provided, ensures consistent ordering and fill missing with <code>0.0</code>.</p> <p>Returns:     Tensor of numeric attribute values</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def transform_attrs(\n    self,\n    attrs: Dict[str, Any],\n    attr_keys: Optional[List[str]] = None,\n) -&gt; Tensor:\n    r\"\"\"\n    Extract and encode numeric attributes to tensor.\n    Non-numeric attributes are discarded. Missing attributes are filled with ``0.0``.\n\n    Args:\n        attrs: Dictionary of attributes\n        attr_keys: Optional list of attribute keys to encode. If provided, ensures consistent ordering and fill missing with ``0.0``.\n\n    Returns:\n        Tensor of numeric attribute values\n    \"\"\"\n    numeric_attrs = {\n        key: value\n        for key, value in attrs.items()\n        if isinstance(value, (int, float)) and not isinstance(value, bool)\n    }\n\n    if attr_keys is not None:\n        values = [float(numeric_attrs.get(key, 0.0)) for key in attr_keys]\n        return torch.tensor(values, dtype=torch.float)\n\n    if not numeric_attrs:\n        return torch.tensor([], dtype=torch.float)\n\n    values = [float(value) for value in numeric_attrs.values()]\n    return torch.tensor(values, dtype=torch.float)\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.__collect_attr_keys","title":"<code>__collect_attr_keys(attr_keys)</code>","text":"<p>Collect unique numeric attribute keys from a list of attribute dictionaries.</p> <p>Args:     attrs_list: List of attribute dictionaries.</p> <p>Returns:     List of unique numeric attribute keys.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def __collect_attr_keys(self, attr_keys: List[Dict[str, Any]]) -&gt; List[str]:\n    \"\"\"\n    Collect unique numeric attribute keys from a list of attribute dictionaries.\n\n    Args:\n        attrs_list: List of attribute dictionaries.\n\n    Returns:\n        List of unique numeric attribute keys.\n    \"\"\"\n    unique_keys = []\n    for attrs in attr_keys:\n        for key, value in attrs.items():\n            if key not in unique_keys and isinstance(value, (int, float)):\n                unique_keys.append(key)\n\n    return unique_keys\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.__get_node_ids_to_sample","title":"<code>__get_node_ids_to_sample(id)</code>","text":"<p>Get a list of node IDs to sample based on the provided index.</p> <p>Args:     id: An integer or a list of integers representing node IDs to sample.</p> <p>Returns:     List of node IDs to sample.</p> <p>Raises:     ValueError: If the provided index is invalid (e.g., empty list or list length exceeds number of nodes).</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def __get_node_ids_to_sample(self, id: int | List[int]) -&gt; List[int]:\n    \"\"\"\n    Get a list of node IDs to sample based on the provided index.\n\n    Args:\n        id: An integer or a list of integers representing node IDs to sample.\n\n    Returns:\n        List of node IDs to sample.\n\n    Raises:\n        ValueError: If the provided index is invalid (e.g., empty list or list length exceeds number of nodes).\n    \"\"\"\n    if isinstance(id, list):\n        if len(id) &lt; 1:\n            raise ValueError(\"Index list cannot be empty.\")\n        elif len(id) &gt; self.__len__():\n            raise ValueError(\n                \"Index list length cannot exceed number of nodes in the hypergraph.\"\n            )\n        return list(set(id))\n\n    return [id]\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.__validate_node_ids","title":"<code>__validate_node_ids(node_ids)</code>","text":"<p>Validate that node IDs are within bounds of the hypergraph.</p> <p>Args:     node_ids: List of node IDs to validate.</p> <p>Raises:     IndexError: If any node ID is out of bounds.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def __validate_node_ids(self, node_ids: List[int]) -&gt; None:\n    \"\"\"\n    Validate that node IDs are within bounds of the hypergraph.\n\n    Args:\n        node_ids: List of node IDs to validate.\n\n    Raises:\n        IndexError: If any node ID is out of bounds.\n    \"\"\"\n    for id in node_ids:\n        if id &lt; 0 or id &gt;= self.__len__():\n            raise IndexError(f\"Node ID {id} is out of bounds (0, {self.__len__() - 1}).\")\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.__new_hyperedge_index","title":"<code>__new_hyperedge_index(sampled_hyperedge_index, sampled_node_ids, sampled_hyperedge_ids)</code>","text":"<p>Create new hyperedge_index with 0-based node and hyperedge IDs.</p> <p>Args:     sampled_hyperedge_index: Original hyperedge_index tensor with sampled incidences.     sampled_node_ids: List of sampled original node IDs.     sampled_hyperedge_ids: List of sampled original hyperedge IDs.</p> <p>Returns:     New hyperedge_index tensor with 0-based node and edge IDs.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def __new_hyperedge_index(\n    self,\n    sampled_hyperedge_index: Tensor,\n    sampled_node_ids: Tensor,\n    sampled_hyperedge_ids: Tensor,\n) -&gt; Tensor:\n    \"\"\"\n    Create new hyperedge_index with 0-based node and hyperedge IDs.\n\n    Args:\n        sampled_hyperedge_index: Original hyperedge_index tensor with sampled incidences.\n        sampled_node_ids: List of sampled original node IDs.\n        sampled_hyperedge_ids: List of sampled original hyperedge IDs.\n\n    Returns:\n        New hyperedge_index tensor with 0-based node and edge IDs.\n    \"\"\"\n    # Example: sampled_edge_index = [[1, 1, 3],\n    #                                [0, 2, 2]]\n    #          sampled_node_ids = [1, 3],\n    #          sampled_edge_ids = [0, 2]\n    #          -&gt; new_node_ids = [0, 0, 1], new_edge_ids = [0, 1, 1]\n    new_node_ids = self.__to_0based_ids(\n        sampled_hyperedge_index[0], sampled_node_ids, self.hdata.num_nodes\n    )\n    new_hyperedge_ids = self.__to_0based_ids(\n        sampled_hyperedge_index[1], sampled_hyperedge_ids, self.hdata.num_edges\n    )\n\n    # Example: new_node_ids = [0, 1], new_hyperedge_ids = [0, 1]\n    #          -&gt; new_hyperedge_index = [[0, 1],\n    #                                    [0, 1]]\n    new_hyperedge_index = torch.stack([new_node_ids, new_hyperedge_ids], dim=0)\n    return new_hyperedge_index\n</code></pre>"},{"location":"api/reference/#hyperbench.data.dataset.Dataset.__to_0based_ids","title":"<code>__to_0based_ids(original_ids, ids_to_keep, n)</code>","text":"<p>Map original IDs to 0-based ids.</p> <p>Example:     original_ids: [1, 3, 3, 7]     ids_to_keep: [3, 7]     n = 8                            # total number of elements (nodes or edges) in the original hypergraph     Returned 0-based IDs: [0, 0, 1]  # the size is sum of occurrences of ids_to_keep in original_ids</p> <p>Args:     original_ids: Tensor of original IDs.     n: Total number of original IDs.     ids_to_keep: List of selected original IDs to be mapped to 0-based.</p> <p>Returns:     Tensor of 0-based ids.</p> Source code in <code>hyperbench/data/dataset.py</code> <pre><code>def __to_0based_ids(\n    self,\n    original_ids: Tensor,\n    ids_to_keep: Tensor,\n    n: int,\n) -&gt; Tensor:\n    \"\"\"\n    Map original IDs to 0-based ids.\n\n    Example:\n        original_ids: [1, 3, 3, 7]\n        ids_to_keep: [3, 7]\n        n = 8                            # total number of elements (nodes or edges) in the original hypergraph\n        Returned 0-based IDs: [0, 0, 1]  # the size is sum of occurrences of ids_to_keep in original_ids\n\n    Args:\n        original_ids: Tensor of original IDs.\n        n: Total number of original IDs.\n        ids_to_keep: List of selected original IDs to be mapped to 0-based.\n\n    Returns:\n        Tensor of 0-based ids.\n    \"\"\"\n    device = original_ids.device\n\n    id_to_0based_id = torch.zeros(n, dtype=torch.long, device=device)\n    n_ids_to_keep = len(ids_to_keep)\n    id_to_0based_id[ids_to_keep] = torch.arange(n_ids_to_keep, device=device)\n    return id_to_0based_id[original_ids]\n</code></pre>"},{"location":"api/reference/#hyperbench.data.loader","title":"<code>hyperbench.data.loader</code>","text":""},{"location":"api/reference/#hyperbench.data.loader.DataLoader","title":"<code>DataLoader</code>","text":"<p>               Bases: <code>DataLoader</code></p> Source code in <code>hyperbench/data/loader.py</code> <pre><code>class DataLoader(TorchDataLoader):\n    def __init__(\n        self,\n        dataset: Dataset,\n        batch_size: int = 1,\n        shuffle: Optional[bool] = False,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            collate_fn=self.collate,\n            **kwargs,\n        )\n\n    def collate(self, batch: List[HData]) -&gt; HData:\n        \"\"\"Collates a list of HData objects into a single batched HData object.\n\n        This function combines multiple separate hypergraph samples into a single\n        batched representation suitable for mini-batch training. It handles:\n        - Concatenating node features from all samples\n        - Concatenating and offsetting hyperedge from all samples\n        - Concatenating edge attributes from all samples, if present\n\n        Example:\n            Given batch = [HData_0, HData_1].\n\n            For node features:\n                HData_0.x.shape = (3, 64) # 3 nodes with 64 features\n                HData_1.x.shape = (2, 64) # 2 nodes with 64 features\n\n                The output will be HData with:\n                    x.shape = (5, 64) # All 5 nodes concatenated\n\n            For hyperedge index:\n                HData_0 (3 nodes, 2 hyperedges):\n                    hyperedge_index = [[0, 1, 1, 2], # Nodes 0, 1, 1, 2\n                                  [0, 0, 1, 1]] # Hyperedge 0 contains {0,1}, Hyperedge 1 contains {1,2}\n\n                HData_1 (2 nodes, 1 hyperedge):\n                    hyperedge_index = [[0, 1], # Nodes 0, 1\n                                  [0, 0]] # Hyperedge 0 contains {0,1}\n\n                Batched result:\n                    hyperedge_index = [[0, 1, 1, 2, 3, 4], # Node indices: original then offset by 3, so 0-&gt;3, 1-&gt;4\n                                  [0, 0, 1, 1, 2, 2]] # Hyperedge IDs: original then offset by 2, so 0-&gt;2, 0-&gt;2\n                                   ^^^^^^^^^^  ^^^^\n                                   Sample 0    Sample 1 (nodes +3, edges +2)\n\n        Args:\n            batch: List of HData objects to collate.\n\n        Returns:\n            HData: A single HData object containing the batched data.\n        \"\"\"\n        node_features, total_nodes = self.__batch_node_features(batch)\n        hyperedge_index, hyperedge_attr, total_hyperedges = self.__batch_hyperedges(batch)\n\n        batched_data = HData(\n            x=node_features,\n            edge_index=hyperedge_index,\n            edge_attr=hyperedge_attr,\n            num_nodes=total_nodes,\n            num_edges=total_hyperedges,\n        )\n\n        return batched_data\n\n    def __batch_node_features(self, batch: List[HData]) -&gt; Tuple[Tensor, int]:\n        \"\"\"Concatenates node features from all samples in the batch.\n\n        Example:\n            With shape being (num_nodes_in_sample, num_features).\n\n            If batch contains 3 sample with node features:\n                Sample 0: x = [[1, 2], [3, 4]]           , shape: (2, 2)\n                Sample 1: x = [[5, 6]]                   , shape: (1, 2)\n                Sample 2: x = [[7, 8], [9, 10], [11, 12]], shape: (3, 2)\n\n            Result:\n                x: [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n                shape: (6, 2), where 6 = 2 + 1 + 3 total nodes.\n\n        Args:\n            batch: List of HData objects.\n\n        Returns:\n            Tensor: Concatenated node features with shape (total_nodes, num_features).\n        \"\"\"\n        per_sample_x = [data.x for data in batch]\n\n        # Stack all nodes along the node dimension from all samples into a single tensor\n        batched_x = torch.cat(per_sample_x, dim=0)\n        total_nodes = batched_x.size(0)\n\n        return batched_x, total_nodes\n\n    def __batch_hyperedges(self, batch: List[HData]) -&gt; Tuple[Tensor, Optional[Tensor], int]:\n        \"\"\"\n        Batches hyperedge indices and attributes, adjusting indices for concatenated nodes.\n        Hyperedge indices must be offset so they point to the correct nodes in the batched node tensor.\n\n        Example:\n            Sample 0 (3 nodes, 2 hyperedges):\n                hyperedge_index = [[0, 1, 1, 2], # Nodes 0, 1, 1, 2\n                                   [0, 0, 1, 1]] # Hyperedge 0 contains {0,1}, Hyperedge 1 contains {1,2}\n                node_offset = 0\n                edge_offset = 0\n\n            Sample 1 (2 nodes, 1 hyperedge):\n                hyperedge_index = [[0, 1], # Nodes 0, 1\n                                   [0, 0]] # Hyperedge 0 contains {0,1}\n                node_offset = 3 # Previous samples have 3 nodes total\n                edge_offset = 2 # Previous samples have 2 hyperedges total\n            Result:\n                hyperedge_index = [[0, 1, 1, 2, 3, 4], # Node indices: original then offset by 3, so 0-&gt;3, 1-&gt;4\n                                   [0, 0, 1, 1, 2, 2]] # Hyperedge IDs: original then offset by 2, so 0-&gt;2, 0-&gt;2\n                                    ^^^^^^^^^^  ^^^^\n                                    Sample 0    Sample 1 (nodes +3, edges +2)\n\n        Args:\n            batch: List of HData objects.\n\n        Returns:\n            Tuple containing:\n                - batched_hyperedge_index: Concatenated and offset hyperedge indices, or ``None``\n                - batched_hyperedge_attr: Concatenated hyperedge attributes, or ``None``\n                - total_hyperedges: Total number of hyperedges across all batched samples\n        \"\"\"\n        hyperedge_indexes = []\n        hyperedge_attrs = []\n        node_offset = 0\n        hyperedge_offset = 0\n\n        for data in batch:\n            # Offset nodes and hyperedge IDs (indices) in hyperedge_index\n            offset_hyperedge_index = data.edge_index.clone()\n            offset_hyperedge_index[0] += node_offset\n            offset_hyperedge_index[1] += hyperedge_offset\n            hyperedge_indexes.append(offset_hyperedge_index)\n\n            if data.edge_attr is not None:\n                hyperedge_attrs.append(data.edge_attr)\n\n            # Offset calculations for next sample based on the max hyperedge ID as it indicates the number of hyperedges\n            max_hyperedge_id = (\n                data.edge_index[1].max().item() if data.edge_index.size(1) &gt; 0 else -1\n            )\n            hyperedge_offset += (\n                data.num_edges if data.num_edges is not None else max_hyperedge_id + 1\n            )\n\n            # Offset calculations for next sample based on x[0] as x has shape (num_nodes, num_features), so 0 provides the number of nodes\n            node_offset += data.num_nodes if data.num_nodes is not None else data.x.size(0)\n\n        # Concatenate all hyperedge_index tensors along the incidence dimension, so that we get a shape of (2, total_hyperedges)\n        batched_hyperedge_index = torch.cat(hyperedge_indexes, dim=1)\n        max_hyperedge_id = int(\n            batched_hyperedge_index[1].max().item() if batched_hyperedge_index.size(1) &gt; 0 else -1\n        )\n        total_hyperedges = max_hyperedge_id + 1\n        batched_hyperedge_attr = None\n        if len(hyperedge_attrs) &gt; 0:\n            # Concatenate hyperedge attributes along dimension 0 (the hyperedge dimension)\n            # hyperedge_attr typically has shape (num_hyperedges, num_hyperedge_features)\n            # Result shape: (total_hyperedges, num_hyperedge_features)\n            batched_hyperedge_attr = torch.cat(hyperedge_attrs, dim=0)\n\n        return batched_hyperedge_index, batched_hyperedge_attr, total_hyperedges\n</code></pre>"},{"location":"api/reference/#hyperbench.data.loader.DataLoader.collate","title":"<code>collate(batch)</code>","text":"<p>Collates a list of HData objects into a single batched HData object.</p> <p>This function combines multiple separate hypergraph samples into a single batched representation suitable for mini-batch training. It handles: - Concatenating node features from all samples - Concatenating and offsetting hyperedge from all samples - Concatenating edge attributes from all samples, if present</p> <p>Example:     Given batch = [HData_0, HData_1].</p> <pre><code>For node features:\n    HData_0.x.shape = (3, 64) # 3 nodes with 64 features\n    HData_1.x.shape = (2, 64) # 2 nodes with 64 features\n\n    The output will be HData with:\n        x.shape = (5, 64) # All 5 nodes concatenated\n\nFor hyperedge index:\n    HData_0 (3 nodes, 2 hyperedges):\n        hyperedge_index = [[0, 1, 1, 2], # Nodes 0, 1, 1, 2\n                      [0, 0, 1, 1]] # Hyperedge 0 contains {0,1}, Hyperedge 1 contains {1,2}\n\n    HData_1 (2 nodes, 1 hyperedge):\n        hyperedge_index = [[0, 1], # Nodes 0, 1\n                      [0, 0]] # Hyperedge 0 contains {0,1}\n\n    Batched result:\n        hyperedge_index = [[0, 1, 1, 2, 3, 4], # Node indices: original then offset by 3, so 0-&gt;3, 1-&gt;4\n                      [0, 0, 1, 1, 2, 2]] # Hyperedge IDs: original then offset by 2, so 0-&gt;2, 0-&gt;2\n                       ^^^^^^^^^^  ^^^^\n                       Sample 0    Sample 1 (nodes +3, edges +2)\n</code></pre> <p>Args:     batch: List of HData objects to collate.</p> <p>Returns:     HData: A single HData object containing the batched data.</p> Source code in <code>hyperbench/data/loader.py</code> <pre><code>def collate(self, batch: List[HData]) -&gt; HData:\n    \"\"\"Collates a list of HData objects into a single batched HData object.\n\n    This function combines multiple separate hypergraph samples into a single\n    batched representation suitable for mini-batch training. It handles:\n    - Concatenating node features from all samples\n    - Concatenating and offsetting hyperedge from all samples\n    - Concatenating edge attributes from all samples, if present\n\n    Example:\n        Given batch = [HData_0, HData_1].\n\n        For node features:\n            HData_0.x.shape = (3, 64) # 3 nodes with 64 features\n            HData_1.x.shape = (2, 64) # 2 nodes with 64 features\n\n            The output will be HData with:\n                x.shape = (5, 64) # All 5 nodes concatenated\n\n        For hyperedge index:\n            HData_0 (3 nodes, 2 hyperedges):\n                hyperedge_index = [[0, 1, 1, 2], # Nodes 0, 1, 1, 2\n                              [0, 0, 1, 1]] # Hyperedge 0 contains {0,1}, Hyperedge 1 contains {1,2}\n\n            HData_1 (2 nodes, 1 hyperedge):\n                hyperedge_index = [[0, 1], # Nodes 0, 1\n                              [0, 0]] # Hyperedge 0 contains {0,1}\n\n            Batched result:\n                hyperedge_index = [[0, 1, 1, 2, 3, 4], # Node indices: original then offset by 3, so 0-&gt;3, 1-&gt;4\n                              [0, 0, 1, 1, 2, 2]] # Hyperedge IDs: original then offset by 2, so 0-&gt;2, 0-&gt;2\n                               ^^^^^^^^^^  ^^^^\n                               Sample 0    Sample 1 (nodes +3, edges +2)\n\n    Args:\n        batch: List of HData objects to collate.\n\n    Returns:\n        HData: A single HData object containing the batched data.\n    \"\"\"\n    node_features, total_nodes = self.__batch_node_features(batch)\n    hyperedge_index, hyperedge_attr, total_hyperedges = self.__batch_hyperedges(batch)\n\n    batched_data = HData(\n        x=node_features,\n        edge_index=hyperedge_index,\n        edge_attr=hyperedge_attr,\n        num_nodes=total_nodes,\n        num_edges=total_hyperedges,\n    )\n\n    return batched_data\n</code></pre>"},{"location":"api/reference/#hyperbench.data.loader.DataLoader.__batch_node_features","title":"<code>__batch_node_features(batch)</code>","text":"<p>Concatenates node features from all samples in the batch.</p> <p>Example:     With shape being (num_nodes_in_sample, num_features).</p> <pre><code>If batch contains 3 sample with node features:\n    Sample 0: x = [[1, 2], [3, 4]]           , shape: (2, 2)\n    Sample 1: x = [[5, 6]]                   , shape: (1, 2)\n    Sample 2: x = [[7, 8], [9, 10], [11, 12]], shape: (3, 2)\n\nResult:\n    x: [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n    shape: (6, 2), where 6 = 2 + 1 + 3 total nodes.\n</code></pre> <p>Args:     batch: List of HData objects.</p> <p>Returns:     Tensor: Concatenated node features with shape (total_nodes, num_features).</p> Source code in <code>hyperbench/data/loader.py</code> <pre><code>def __batch_node_features(self, batch: List[HData]) -&gt; Tuple[Tensor, int]:\n    \"\"\"Concatenates node features from all samples in the batch.\n\n    Example:\n        With shape being (num_nodes_in_sample, num_features).\n\n        If batch contains 3 sample with node features:\n            Sample 0: x = [[1, 2], [3, 4]]           , shape: (2, 2)\n            Sample 1: x = [[5, 6]]                   , shape: (1, 2)\n            Sample 2: x = [[7, 8], [9, 10], [11, 12]], shape: (3, 2)\n\n        Result:\n            x: [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n            shape: (6, 2), where 6 = 2 + 1 + 3 total nodes.\n\n    Args:\n        batch: List of HData objects.\n\n    Returns:\n        Tensor: Concatenated node features with shape (total_nodes, num_features).\n    \"\"\"\n    per_sample_x = [data.x for data in batch]\n\n    # Stack all nodes along the node dimension from all samples into a single tensor\n    batched_x = torch.cat(per_sample_x, dim=0)\n    total_nodes = batched_x.size(0)\n\n    return batched_x, total_nodes\n</code></pre>"},{"location":"api/reference/#hyperbench.data.loader.DataLoader.__batch_hyperedges","title":"<code>__batch_hyperedges(batch)</code>","text":"<p>Batches hyperedge indices and attributes, adjusting indices for concatenated nodes. Hyperedge indices must be offset so they point to the correct nodes in the batched node tensor.</p> <p>Example:     Sample 0 (3 nodes, 2 hyperedges):         hyperedge_index = [[0, 1, 1, 2], # Nodes 0, 1, 1, 2                            [0, 0, 1, 1]] # Hyperedge 0 contains {0,1}, Hyperedge 1 contains {1,2}         node_offset = 0         edge_offset = 0</p> <pre><code>Sample 1 (2 nodes, 1 hyperedge):\n    hyperedge_index = [[0, 1], # Nodes 0, 1\n                       [0, 0]] # Hyperedge 0 contains {0,1}\n    node_offset = 3 # Previous samples have 3 nodes total\n    edge_offset = 2 # Previous samples have 2 hyperedges total\nResult:\n    hyperedge_index = [[0, 1, 1, 2, 3, 4], # Node indices: original then offset by 3, so 0-&gt;3, 1-&gt;4\n                       [0, 0, 1, 1, 2, 2]] # Hyperedge IDs: original then offset by 2, so 0-&gt;2, 0-&gt;2\n                        ^^^^^^^^^^  ^^^^\n                        Sample 0    Sample 1 (nodes +3, edges +2)\n</code></pre> <p>Args:     batch: List of HData objects.</p> <p>Returns:     Tuple containing:         - batched_hyperedge_index: Concatenated and offset hyperedge indices, or <code>None</code>         - batched_hyperedge_attr: Concatenated hyperedge attributes, or <code>None</code>         - total_hyperedges: Total number of hyperedges across all batched samples</p> Source code in <code>hyperbench/data/loader.py</code> <pre><code>def __batch_hyperedges(self, batch: List[HData]) -&gt; Tuple[Tensor, Optional[Tensor], int]:\n    \"\"\"\n    Batches hyperedge indices and attributes, adjusting indices for concatenated nodes.\n    Hyperedge indices must be offset so they point to the correct nodes in the batched node tensor.\n\n    Example:\n        Sample 0 (3 nodes, 2 hyperedges):\n            hyperedge_index = [[0, 1, 1, 2], # Nodes 0, 1, 1, 2\n                               [0, 0, 1, 1]] # Hyperedge 0 contains {0,1}, Hyperedge 1 contains {1,2}\n            node_offset = 0\n            edge_offset = 0\n\n        Sample 1 (2 nodes, 1 hyperedge):\n            hyperedge_index = [[0, 1], # Nodes 0, 1\n                               [0, 0]] # Hyperedge 0 contains {0,1}\n            node_offset = 3 # Previous samples have 3 nodes total\n            edge_offset = 2 # Previous samples have 2 hyperedges total\n        Result:\n            hyperedge_index = [[0, 1, 1, 2, 3, 4], # Node indices: original then offset by 3, so 0-&gt;3, 1-&gt;4\n                               [0, 0, 1, 1, 2, 2]] # Hyperedge IDs: original then offset by 2, so 0-&gt;2, 0-&gt;2\n                                ^^^^^^^^^^  ^^^^\n                                Sample 0    Sample 1 (nodes +3, edges +2)\n\n    Args:\n        batch: List of HData objects.\n\n    Returns:\n        Tuple containing:\n            - batched_hyperedge_index: Concatenated and offset hyperedge indices, or ``None``\n            - batched_hyperedge_attr: Concatenated hyperedge attributes, or ``None``\n            - total_hyperedges: Total number of hyperedges across all batched samples\n    \"\"\"\n    hyperedge_indexes = []\n    hyperedge_attrs = []\n    node_offset = 0\n    hyperedge_offset = 0\n\n    for data in batch:\n        # Offset nodes and hyperedge IDs (indices) in hyperedge_index\n        offset_hyperedge_index = data.edge_index.clone()\n        offset_hyperedge_index[0] += node_offset\n        offset_hyperedge_index[1] += hyperedge_offset\n        hyperedge_indexes.append(offset_hyperedge_index)\n\n        if data.edge_attr is not None:\n            hyperedge_attrs.append(data.edge_attr)\n\n        # Offset calculations for next sample based on the max hyperedge ID as it indicates the number of hyperedges\n        max_hyperedge_id = (\n            data.edge_index[1].max().item() if data.edge_index.size(1) &gt; 0 else -1\n        )\n        hyperedge_offset += (\n            data.num_edges if data.num_edges is not None else max_hyperedge_id + 1\n        )\n\n        # Offset calculations for next sample based on x[0] as x has shape (num_nodes, num_features), so 0 provides the number of nodes\n        node_offset += data.num_nodes if data.num_nodes is not None else data.x.size(0)\n\n    # Concatenate all hyperedge_index tensors along the incidence dimension, so that we get a shape of (2, total_hyperedges)\n    batched_hyperedge_index = torch.cat(hyperedge_indexes, dim=1)\n    max_hyperedge_id = int(\n        batched_hyperedge_index[1].max().item() if batched_hyperedge_index.size(1) &gt; 0 else -1\n    )\n    total_hyperedges = max_hyperedge_id + 1\n    batched_hyperedge_attr = None\n    if len(hyperedge_attrs) &gt; 0:\n        # Concatenate hyperedge attributes along dimension 0 (the hyperedge dimension)\n        # hyperedge_attr typically has shape (num_hyperedges, num_hyperedge_features)\n        # Result shape: (total_hyperedges, num_hyperedge_features)\n        batched_hyperedge_attr = torch.cat(hyperedge_attrs, dim=0)\n\n    return batched_hyperedge_index, batched_hyperedge_attr, total_hyperedges\n</code></pre>"},{"location":"api/reference/#train-module","title":"Train Module","text":""},{"location":"api/reference/#trainer","title":"Trainer","text":""},{"location":"api/reference/#hyperbench.train.trainer","title":"<code>hyperbench.train.trainer</code>","text":""},{"location":"api/reference/#hyperbench.train.trainer.MultiModelTrainer","title":"<code>MultiModelTrainer</code>","text":"<p>A trainer class to handle training multiple models with individual trainers.</p> <p>Args:     accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")         as well as custom accelerator instances.</p> <pre><code>devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n    (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n    automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n\nstrategy: Supports different training strategies with aliases as well custom strategies.\n    Default: ``\"auto\"``.\n\nnum_nodes: Number of GPU nodes for distributed training.\n    Default: ``1``.\n\nprecision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n    16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n    Can be used on CPU, GPU, TPUs, or HPUs.\n    Default: ``'32-true'``.\n\nmax_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n    If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n    To enable infinite training, set ``max_epochs = -1``.\n\nmin_epochs: Force training for at least these many epochs. Disabled by default (None).\n\nmax_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n    and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n    ``max_epochs`` to ``-1``.\n\nmin_steps: Force training for at least these number of steps. Disabled by default (``None``).\n\ncheck_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,\n    validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n    to be an integer value. When used together with a time-based ``val_check_interval`` and\n    ``check_val_every_n_epoch`` &gt; 1, validation is aligned to epoch multiples: if the interval elapses\n    before the next multiple-N epoch, validation runs at the start of that epoch (after the first batch)\n    and the timer resets; if it elapses during a multiple-N epoch, validation runs after the current batch.\n    For ``None`` or ``1`` cases, the time-based behavior of ``val_check_interval`` applies without\n    additional alignment.\n    Default: ``1``.\n\nlogger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n    the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n    ``False`` will disable logging. If multiple loggers are provided, local files\n    (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n    Default: ``True``.\n\ndefault_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n    Default: ``os.getcwd()``.\n    Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n\nenable_autolog_hparams: Whether to log hyperparameters at the start of a run.\n    Default: ``True``.\n\nlog_every_n_steps: How often to log within steps.\n    Default: ``50``.\n\nprofiler: To profile individual steps during training and assist in identifying bottlenecks.\n    Default: ``None``.\n\nfast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n    of train, val and test to find any bugs (ie: a sort of unit test).\n    Default: ``False``.\n\nenable_checkpointing: If ``True``, enable checkpointing.\n    It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n    :paramref:`~hyperbench.train.MultiModelTrainer.callbacks`.\n    Default: ``True``.\n\nenable_progress_bar: Whether to enable the progress bar by default.\n    Default: ``True``.\n\nenable_model_summary: Whether to enable model summarization by default.\n    Default: ``True``.\n\ncallbacks: Add a callback or list of callbacks.\n    Default: ``None``.\n</code></pre> Source code in <code>hyperbench/train/trainer.py</code> <pre><code>class MultiModelTrainer:\n    r\"\"\"\n    A trainer class to handle training multiple models with individual trainers.\n\n    Args:\n        accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")\n            as well as custom accelerator instances.\n\n        devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n            (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n            automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n\n        strategy: Supports different training strategies with aliases as well custom strategies.\n            Default: ``\"auto\"``.\n\n        num_nodes: Number of GPU nodes for distributed training.\n            Default: ``1``.\n\n        precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n            16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n            Can be used on CPU, GPU, TPUs, or HPUs.\n            Default: ``'32-true'``.\n\n        max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n            If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n            To enable infinite training, set ``max_epochs = -1``.\n\n        min_epochs: Force training for at least these many epochs. Disabled by default (None).\n\n        max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n            and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n            ``max_epochs`` to ``-1``.\n\n        min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n\n        check_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,\n            validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n            to be an integer value. When used together with a time-based ``val_check_interval`` and\n            ``check_val_every_n_epoch`` &gt; 1, validation is aligned to epoch multiples: if the interval elapses\n            before the next multiple-N epoch, validation runs at the start of that epoch (after the first batch)\n            and the timer resets; if it elapses during a multiple-N epoch, validation runs after the current batch.\n            For ``None`` or ``1`` cases, the time-based behavior of ``val_check_interval`` applies without\n            additional alignment.\n            Default: ``1``.\n\n        logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n            the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n            ``False`` will disable logging. If multiple loggers are provided, local files\n            (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n            Default: ``True``.\n\n        default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n            Default: ``os.getcwd()``.\n            Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n\n        enable_autolog_hparams: Whether to log hyperparameters at the start of a run.\n            Default: ``True``.\n\n        log_every_n_steps: How often to log within steps.\n            Default: ``50``.\n\n        profiler: To profile individual steps during training and assist in identifying bottlenecks.\n            Default: ``None``.\n\n        fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n            of train, val and test to find any bugs (ie: a sort of unit test).\n            Default: ``False``.\n\n        enable_checkpointing: If ``True``, enable checkpointing.\n            It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n            :paramref:`~hyperbench.train.MultiModelTrainer.callbacks`.\n            Default: ``True``.\n\n        enable_progress_bar: Whether to enable the progress bar by default.\n            Default: ``True``.\n\n        enable_model_summary: Whether to enable model summarization by default.\n            Default: ``True``.\n\n        callbacks: Add a callback or list of callbacks.\n            Default: ``None``.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_configs: List[ModelConfig],\n        # args to pass to each Trainer\n        accelerator: str | Accelerator = \"auto\",\n        devices: list[int] | str | int = \"auto\",\n        strategy: str | Strategy = \"auto\",\n        num_nodes: int = 1,\n        precision: Optional[\n            Any  # Any as Lightning accepts multiple types (int, str, Literal, etc.)\n        ] = None,\n        max_epochs: Optional[int] = None,\n        min_epochs: Optional[int] = None,\n        max_steps: int = -1,\n        min_steps: Optional[int] = None,\n        check_val_every_n_epoch: Optional[int] = 1,\n        logger: Optional[Logger | Iterable[Logger] | bool] = None,\n        default_root_dir: Optional[str | Path] = None,\n        enable_autolog_hparams: bool = True,\n        log_every_n_steps: Optional[int] = None,\n        profiler: Optional[Profiler | str] = None,\n        fast_dev_run: int | bool = False,\n        enable_checkpointing: bool = True,\n        enable_progress_bar: bool = True,\n        enable_model_summary: Optional[bool] = None,\n        callbacks: Optional[List[Callback] | Callback] = None,\n        **kwargs,\n    ) -&gt; None:\n        self.model_configs: List[ModelConfig] = model_configs\n\n        for model_config in model_configs:\n            if model_config.trainer is None:\n                model_config.trainer = L.Trainer(\n                    accelerator=accelerator,\n                    devices=devices,\n                    strategy=strategy,\n                    num_nodes=num_nodes,\n                    precision=precision,\n                    max_epochs=max_epochs,\n                    min_epochs=min_epochs,\n                    max_steps=max_steps,\n                    min_steps=min_steps,\n                    check_val_every_n_epoch=check_val_every_n_epoch,\n                    logger=logger,\n                    default_root_dir=default_root_dir,\n                    enable_autolog_hparams=enable_autolog_hparams,\n                    log_every_n_steps=log_every_n_steps,\n                    profiler=profiler,\n                    fast_dev_run=fast_dev_run,\n                    enable_checkpointing=enable_checkpointing,\n                    enable_progress_bar=enable_progress_bar,\n                    enable_model_summary=enable_model_summary,\n                    callbacks=callbacks,\n                    **kwargs,\n                )\n\n    @property\n    def models(self) -&gt; List[L.LightningModule]:\n        return [config.model for config in self.model_configs]\n\n    def model(self, name: str, version: str = \"default\") -&gt; Optional[L.LightningModule]:\n        for config in self.model_configs:\n            if config.name == name and config.version == version:\n                return config.model\n        return None\n\n    def fit_all(\n        self,\n        train_dataloader: Optional[DataLoader] = None,\n        val_dataloader: Optional[DataLoader] = None,\n        datamodule: Optional[L.LightningDataModule] = None,\n        ckpt_path: Optional[CkptStrategy] = None,\n        verbose: bool = True,\n    ) -&gt; None:\n        if len(self.model_configs) &lt; 1:\n            raise ValueError(\"No models to fit.\")\n\n        for i, config in enumerate(self.model_configs):\n            if config.trainer is None:\n                raise ValueError(f\"Trainer not defined for model {config.full_model_name()}.\")\n\n            if verbose:\n                log.info(\n                    f\"Fit model {config.full_model_name()} [{i + 1}/{len(self.model_configs)}]\\n\"\n                )\n\n            config.trainer.fit(\n                model=config.model,\n                train_dataloaders=train_dataloader,\n                val_dataloaders=val_dataloader,\n                datamodule=datamodule,\n                ckpt_path=ckpt_path,\n            )\n\n    def test_all(\n        self,\n        dataloader: Optional[DataLoader] = None,\n        datamodule: Optional[L.LightningDataModule] = None,\n        ckpt_path: Optional[CkptStrategy] = None,\n        verbose: bool = True,\n        verbose_loop: bool = True,\n    ) -&gt; Mapping[str, TestResult]:\n        if len(self.model_configs) &lt; 1:\n            raise ValueError(\"No models to test.\")\n\n        test_results: Dict[str, TestResult] = {}\n\n        for i, config in enumerate(self.model_configs):\n            if config.trainer is None:\n                raise ValueError(f\"Trainer not defined for model {config.full_model_name()}.\")\n\n            if verbose:\n                log.info(\n                    f\"Test model {config.full_model_name()} [{i + 1}/{len(self.model_configs)}]\\n\"\n                )\n\n            trainer_test_results: List[TestResult] = config.trainer.test(\n                model=config.model,\n                dataloaders=dataloader,\n                datamodule=datamodule,\n                ckpt_path=ckpt_path,\n                verbose=verbose_loop,\n            )\n\n            # In Lightning, test() returns a list of dicts, one per dataloader, but we use a single dataloader\n            test_results[config.full_model_name()] = (\n                trainer_test_results[0] if len(trainer_test_results) &gt; 0 else {}\n            )\n\n        return test_results\n</code></pre>"},{"location":"api/reference/#negative-sampler","title":"Negative Sampler","text":""},{"location":"api/reference/#hyperbench.train.negative_sampler","title":"<code>hyperbench.train.negative_sampler</code>","text":""},{"location":"api/reference/#hyperbench.train.negative_sampler.NegativeSampler","title":"<code>NegativeSampler</code>","text":"Source code in <code>hyperbench/train/negative_sampler.py</code> <pre><code>class NegativeSampler:\n    def sample(self, data: HData) -&gt; HData:\n        \"\"\"\n        Abstract method for negative sampling.\n\n        Args:\n            data: HData\n                The input data object containing graph or hypergraph information.\n\n        Returns:\n            HData: The negative samples as a new HData object.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n</code></pre>"},{"location":"api/reference/#hyperbench.train.negative_sampler.NegativeSampler.sample","title":"<code>sample(data)</code>","text":"<p>Abstract method for negative sampling.</p> <p>Args:     data: HData         The input data object containing graph or hypergraph information.</p> <p>Returns:     HData: The negative samples as a new HData object.</p> <p>Raises:     NotImplementedError: If the method is not implemented in a subclass.</p> Source code in <code>hyperbench/train/negative_sampler.py</code> <pre><code>def sample(self, data: HData) -&gt; HData:\n    \"\"\"\n    Abstract method for negative sampling.\n\n    Args:\n        data: HData\n            The input data object containing graph or hypergraph information.\n\n    Returns:\n        HData: The negative samples as a new HData object.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement this method.\")\n</code></pre>"},{"location":"api/reference/#hyperbench.train.negative_sampler.RandomNegativeSampler","title":"<code>RandomNegativeSampler</code>","text":"<p>               Bases: <code>NegativeSampler</code></p> <p>A random negative sampler.</p> <p>Args:     num_negative_samples (int): Number of negative hyperedges to generate.     num_nodes_per_sample (int): Number of nodes per negative hyperedge.</p> <p>Raises:     ValueError: If either argument is not positive.</p> Source code in <code>hyperbench/train/negative_sampler.py</code> <pre><code>class RandomNegativeSampler(NegativeSampler):\n    \"\"\"\n    A random negative sampler.\n\n    Args:\n        num_negative_samples (int): Number of negative hyperedges to generate.\n        num_nodes_per_sample (int): Number of nodes per negative hyperedge.\n\n    Raises:\n        ValueError: If either argument is not positive.\n    \"\"\"\n\n    def __init__(self, num_negative_samples: int, num_nodes_per_sample: int):\n        if num_negative_samples &lt;= 0:\n            raise ValueError(f\"num_negative_samples must be positive, got {num_negative_samples}.\")\n        if num_nodes_per_sample &lt;= 0:\n            raise ValueError(f\"num_nodes_per_sample must be positive, got {num_nodes_per_sample}.\")\n\n        super().__init__()\n        self.num_negative_samples = num_negative_samples\n        self.num_nodes_per_sample = num_nodes_per_sample\n\n    def sample(self, data: HData) -&gt; HData:\n        \"\"\"\n        Generate negative hyperedges by randomly sampling unique node IDs.\n\n        Args:\n            data (HData): The input data object containing node and hyperedge information.\n\n        Returns:\n            HData: A new HData object containing the negative samples.\n\n        Raises:\n            ValueError: If num_nodes_per_sample is greater than the number of available nodes.\n        \"\"\"\n        if self.num_nodes_per_sample &gt; data.num_nodes:\n            raise ValueError(\n                f\"Asked to create samples with {self.num_nodes_per_sample} nodes, but only {data.num_nodes} nodes are available.\"\n            )\n\n        negative_node_ids: Set[int] = set()\n        sampled_hyperedge_indexes: List[Tensor] = []\n        sampled_hyperedge_attrs: List[Tensor] = []\n\n        device = data.x.device\n        new_hyperedge_id_offset = data.num_edges\n        for new_hyperedge_id in range(self.num_negative_samples):\n            # Sample with multinomial without replacement to ensure unique node ids\n            # and assign each node id equal probability of being selected by setting all of them to 1\n            # Example: num_nodes_per_sample=3, max_node_id=5\n            #          -&gt; possible output: [2, 0, 4]\n            equal_probabilities = torch.ones(data.num_nodes, device=device)\n            sampled_node_ids = torch.multinomial(\n                equal_probabilities, self.num_nodes_per_sample, replacement=False\n            )\n\n            # Example: sampled_node_ids = [2, 0, 4], new_hyperedge_id=0, new_hyperedge_id_offset=3\n            #          -&gt; hyperedge_index = [[2, 0, 4],\n            #                                [3, 3, 3]]\n            sampled_hyperedge_id_tensor = torch.full(\n                (self.num_nodes_per_sample,),\n                new_hyperedge_id + new_hyperedge_id_offset,\n                device=device,\n            )\n            sampled_hyperedge_index = torch.stack(\n                [sampled_node_ids, sampled_hyperedge_id_tensor], dim=0\n            )\n            sampled_hyperedge_indexes.append(sampled_hyperedge_index)\n\n            # Example: nodes = [0, 1, 2],\n            #          sampled_node_ids_0 = [0, 1], sampled_node_ids_1 = [1, 2],\n            #          -&gt; negative_node_ids = {0, 1, 2}\n            negative_node_ids.update(sampled_node_ids.tolist())\n\n            if data.edge_attr is not None:\n                random_edge_attr = torch.randn_like(data.edge_attr[0])\n                sampled_hyperedge_attrs.append(random_edge_attr)\n\n        negative_x = data.x[sorted(negative_node_ids)]\n        negative_hyperedge_index = self.__new_negative_hyperedge_index(sampled_hyperedge_indexes)\n        negative_hyperedge_attr = (\n            torch.stack(sampled_hyperedge_attrs, dim=0) if data.edge_attr is not None else None\n        )\n\n        return HData(\n            x=negative_x,\n            edge_index=negative_hyperedge_index,\n            edge_attr=negative_hyperedge_attr,\n            num_nodes=len(negative_node_ids),\n            num_edges=self.num_negative_samples,\n        )\n\n    def __new_negative_hyperedge_index(self, sampled_hyperedge_indexes: List[Tensor]) -&gt; Tensor:\n        \"\"\"\n        Concatenate and sort the sampled hyperedge indexes for negative samples.\n\n        Args:\n            sampled_hyperedge_indexes (Tensor): List of hyperedge index tensors for each negative sample.\n\n        Returns:\n            Tensor: The concatenated and sorted hyperedge index tensor.\n        \"\"\"\n        negative_hyperedge_index = torch.cat(sampled_hyperedge_indexes, dim=1)\n        node_ids_order = negative_hyperedge_index[0].argsort()\n\n        # Example: negative_hyperedge_index before sorting: [[2, 0, 4, 0, 1, 3],\n        #                                                    [3, 3, 3, 4, 4, 4]]\n        #          -&gt; negative_hyperedge_index after sorting: [[0, 0, 1, 2, 3, 4],\n        #                                                      [3, 4, 4, 3, 4, 3]]\n        negative_hyperedge_index = negative_hyperedge_index[:, node_ids_order]\n        return negative_hyperedge_index\n</code></pre>"},{"location":"api/reference/#hyperbench.train.negative_sampler.RandomNegativeSampler.sample","title":"<code>sample(data)</code>","text":"<p>Generate negative hyperedges by randomly sampling unique node IDs.</p> <p>Args:     data (HData): The input data object containing node and hyperedge information.</p> <p>Returns:     HData: A new HData object containing the negative samples.</p> <p>Raises:     ValueError: If num_nodes_per_sample is greater than the number of available nodes.</p> Source code in <code>hyperbench/train/negative_sampler.py</code> <pre><code>def sample(self, data: HData) -&gt; HData:\n    \"\"\"\n    Generate negative hyperedges by randomly sampling unique node IDs.\n\n    Args:\n        data (HData): The input data object containing node and hyperedge information.\n\n    Returns:\n        HData: A new HData object containing the negative samples.\n\n    Raises:\n        ValueError: If num_nodes_per_sample is greater than the number of available nodes.\n    \"\"\"\n    if self.num_nodes_per_sample &gt; data.num_nodes:\n        raise ValueError(\n            f\"Asked to create samples with {self.num_nodes_per_sample} nodes, but only {data.num_nodes} nodes are available.\"\n        )\n\n    negative_node_ids: Set[int] = set()\n    sampled_hyperedge_indexes: List[Tensor] = []\n    sampled_hyperedge_attrs: List[Tensor] = []\n\n    device = data.x.device\n    new_hyperedge_id_offset = data.num_edges\n    for new_hyperedge_id in range(self.num_negative_samples):\n        # Sample with multinomial without replacement to ensure unique node ids\n        # and assign each node id equal probability of being selected by setting all of them to 1\n        # Example: num_nodes_per_sample=3, max_node_id=5\n        #          -&gt; possible output: [2, 0, 4]\n        equal_probabilities = torch.ones(data.num_nodes, device=device)\n        sampled_node_ids = torch.multinomial(\n            equal_probabilities, self.num_nodes_per_sample, replacement=False\n        )\n\n        # Example: sampled_node_ids = [2, 0, 4], new_hyperedge_id=0, new_hyperedge_id_offset=3\n        #          -&gt; hyperedge_index = [[2, 0, 4],\n        #                                [3, 3, 3]]\n        sampled_hyperedge_id_tensor = torch.full(\n            (self.num_nodes_per_sample,),\n            new_hyperedge_id + new_hyperedge_id_offset,\n            device=device,\n        )\n        sampled_hyperedge_index = torch.stack(\n            [sampled_node_ids, sampled_hyperedge_id_tensor], dim=0\n        )\n        sampled_hyperedge_indexes.append(sampled_hyperedge_index)\n\n        # Example: nodes = [0, 1, 2],\n        #          sampled_node_ids_0 = [0, 1], sampled_node_ids_1 = [1, 2],\n        #          -&gt; negative_node_ids = {0, 1, 2}\n        negative_node_ids.update(sampled_node_ids.tolist())\n\n        if data.edge_attr is not None:\n            random_edge_attr = torch.randn_like(data.edge_attr[0])\n            sampled_hyperedge_attrs.append(random_edge_attr)\n\n    negative_x = data.x[sorted(negative_node_ids)]\n    negative_hyperedge_index = self.__new_negative_hyperedge_index(sampled_hyperedge_indexes)\n    negative_hyperedge_attr = (\n        torch.stack(sampled_hyperedge_attrs, dim=0) if data.edge_attr is not None else None\n    )\n\n    return HData(\n        x=negative_x,\n        edge_index=negative_hyperedge_index,\n        edge_attr=negative_hyperedge_attr,\n        num_nodes=len(negative_node_ids),\n        num_edges=self.num_negative_samples,\n    )\n</code></pre>"},{"location":"api/reference/#hyperbench.train.negative_sampler.RandomNegativeSampler.__new_negative_hyperedge_index","title":"<code>__new_negative_hyperedge_index(sampled_hyperedge_indexes)</code>","text":"<p>Concatenate and sort the sampled hyperedge indexes for negative samples.</p> <p>Args:     sampled_hyperedge_indexes (Tensor): List of hyperedge index tensors for each negative sample.</p> <p>Returns:     Tensor: The concatenated and sorted hyperedge index tensor.</p> Source code in <code>hyperbench/train/negative_sampler.py</code> <pre><code>def __new_negative_hyperedge_index(self, sampled_hyperedge_indexes: List[Tensor]) -&gt; Tensor:\n    \"\"\"\n    Concatenate and sort the sampled hyperedge indexes for negative samples.\n\n    Args:\n        sampled_hyperedge_indexes (Tensor): List of hyperedge index tensors for each negative sample.\n\n    Returns:\n        Tensor: The concatenated and sorted hyperedge index tensor.\n    \"\"\"\n    negative_hyperedge_index = torch.cat(sampled_hyperedge_indexes, dim=1)\n    node_ids_order = negative_hyperedge_index[0].argsort()\n\n    # Example: negative_hyperedge_index before sorting: [[2, 0, 4, 0, 1, 3],\n    #                                                    [3, 3, 3, 4, 4, 4]]\n    #          -&gt; negative_hyperedge_index after sorting: [[0, 0, 1, 2, 3, 4],\n    #                                                      [3, 4, 4, 3, 4, 3]]\n    negative_hyperedge_index = negative_hyperedge_index[:, node_ids_order]\n    return negative_hyperedge_index\n</code></pre>"},{"location":"api/reference/#types-module","title":"Types Module","text":""},{"location":"api/reference/#hdata","title":"HData","text":""},{"location":"api/reference/#hyperbench.types.hdata","title":"<code>hyperbench.types.hdata</code>","text":""},{"location":"api/reference/#hyperbench.types.hdata.HData","title":"<code>HData</code>","text":"<p>Container for hypergraph data.</p> <p>Attributes:     x (Tensor): Node feature matrix of shape [num_nodes, num_features].     edge_index (Tensor): Hyperedge connectivity in COO format         of shape [2, num_incidences], where edge_index[0] contains         node IDs and edge_index[1] contains hyperedge IDs.     edge_attr (Tensor, optional): Hyperedge feature matrix of shape         [num_edges, num_edge_features]. Features associated with each         hyperedge (e.g., weights, timestamps, types).     num_nodes (int, optional): Number of nodes in the hypergraph.         If None, inferred as x.size(0).     num_edges (int, optional): Number of hyperedges in the hypergraph.         If None, inferred as edge_index[1].max().item() + 1.</p> <p>Example:     &gt;&gt;&gt; x = torch.randn(10, 16)  # 10 nodes with 16 features each     &gt;&gt;&gt; edge_index = torch.tensor([[0, 0, 1, 1, 1],  # node IDs     ...                            [0, 1, 2, 3, 4]]) # hyperedge IDs     &gt;&gt;&gt; data = HData(x, edge_index=edge_index)</p> Source code in <code>hyperbench/types/hdata.py</code> <pre><code>class HData:\n    \"\"\"\n    Container for hypergraph data.\n\n    Attributes:\n        x (Tensor): Node feature matrix of shape [num_nodes, num_features].\n        edge_index (Tensor): Hyperedge connectivity in COO format\n            of shape [2, num_incidences], where edge_index[0] contains\n            node IDs and edge_index[1] contains hyperedge IDs.\n        edge_attr (Tensor, optional): Hyperedge feature matrix of shape\n            [num_edges, num_edge_features]. Features associated with each\n            hyperedge (e.g., weights, timestamps, types).\n        num_nodes (int, optional): Number of nodes in the hypergraph.\n            If None, inferred as x.size(0).\n        num_edges (int, optional): Number of hyperedges in the hypergraph.\n            If None, inferred as edge_index[1].max().item() + 1.\n\n    Example:\n        &gt;&gt;&gt; x = torch.randn(10, 16)  # 10 nodes with 16 features each\n        &gt;&gt;&gt; edge_index = torch.tensor([[0, 0, 1, 1, 1],  # node IDs\n        ...                            [0, 1, 2, 3, 4]]) # hyperedge IDs\n        &gt;&gt;&gt; data = HData(x, edge_index=edge_index)\n    \"\"\"\n\n    def __init__(\n        self,\n        x: Tensor,\n        edge_index: Tensor,\n        edge_attr: Optional[Tensor] = None,\n        num_nodes: Optional[int] = None,\n        num_edges: Optional[int] = None,\n    ):\n        self.x: Tensor = x\n\n        self.edge_index: Tensor = edge_index\n\n        self.edge_attr: Optional[Tensor] = edge_attr\n\n        self.num_nodes: int = num_nodes if num_nodes is not None else x.size(0)\n\n        max_edge_id = edge_index[1].max().item() if edge_index.size(1) &gt; 0 else -1\n        self.num_edges: int = num_edges if num_edges is not None else max_edge_id + 1\n\n    @classmethod\n    def empty(cls) -&gt; \"HData\":\n        return cls(\n            x=utils.empty_nodefeatures(),\n            edge_index=utils.empty_edgeindex(),\n            edge_attr=None,\n            num_nodes=0,\n            num_edges=0,\n        )\n\n    def to(self, device: device | str, non_blocking: bool = False) -&gt; \"HData\":\n        self.x = self.x.to(device=device, non_blocking=non_blocking)\n        self.edge_index = self.edge_index.to(device=device, non_blocking=non_blocking)\n        if self.edge_attr is not None:\n            self.edge_attr = self.edge_attr.to(device=device, non_blocking=non_blocking)\n        return self\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\\n\"\n            f\"    num_nodes={self.num_nodes},\\n\"\n            f\"    num_edges={self.num_edges},\\n\"\n            f\"    x_shape={self.x.shape},\\n\"\n            f\"    edge_index_shape={self.edge_index.shape},\\n\"\n            f\"    edge_attr_shape={self.edge_attr.shape if self.edge_attr is not None else None}\\n\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/reference/#hypergraph","title":"Hypergraph","text":""},{"location":"api/reference/#hyperbench.types.hypergraph","title":"<code>hyperbench.types.hypergraph</code>","text":""},{"location":"api/reference/#hyperbench.types.hypergraph.HIFHypergraph","title":"<code>HIFHypergraph</code>","text":"<p>A hypergraph data structure that supports directed/undirected hyperedges with incidence-based representation.</p> Source code in <code>hyperbench/types/hypergraph.py</code> <pre><code>class HIFHypergraph:\n    \"\"\"\n    A hypergraph data structure that supports directed/undirected hyperedges\n    with incidence-based representation.\n    \"\"\"\n\n    def __init__(\n        self,\n        network_type: Optional[Literal[\"asc\", \"directed\", \"undirected\"]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        incidences: Optional[List[Dict[str, Any]]] = None,\n        nodes: Optional[List[Dict[str, Any]]] = None,\n        edges: Optional[List[Dict[str, Any]]] = None,\n    ):\n        self.network_type = network_type\n        self.metadata = metadata if metadata is not None else {}\n        self.incidences = incidences if incidences is not None else []\n        self.nodes = nodes if nodes is not None else []\n        self.edges = edges if edges is not None else []\n\n    @classmethod\n    def empty(cls) -&gt; \"HIFHypergraph\":\n        return cls(\n            network_type=\"undirected\",\n            nodes=[],\n            edges=[],\n            incidences=[],\n            metadata=None,\n        )\n\n    @classmethod\n    def from_hif(cls, data: Dict[str, Any]) -&gt; \"HIFHypergraph\":\n        \"\"\"\n        Create a Hypergraph from a HIF (Hypergraph Interchange Format).\n\n        Args:\n            data: Dictionary with keys: network-type, metadata, incidences, nodes, edges\n\n        Returns:\n            Hypergraph instance\n        \"\"\"\n        network_type = data.get(\"network-type\") or data.get(\"network_type\")\n        metadata = data.get(\"metadata\", {})\n        incidences = data.get(\"incidences\", [])\n        nodes = data.get(\"nodes\", [])\n        edges = data.get(\"edges\", [])\n\n        return cls(\n            network_type=network_type,\n            metadata=metadata,\n            incidences=incidences,\n            nodes=nodes,\n            edges=edges,\n        )\n\n    @property\n    def num_nodes(self) -&gt; int:\n        \"\"\"Return the number of nodes in the hypergraph.\"\"\"\n        return len(self.nodes)\n\n    @property\n    def num_edges(self) -&gt; int:\n        \"\"\"Return the number of edges in the hypergraph.\"\"\"\n        return len(self.edges)\n</code></pre>"},{"location":"api/reference/#hyperbench.types.hypergraph.HIFHypergraph.num_nodes","title":"<code>num_nodes</code>  <code>property</code>","text":"<p>Return the number of nodes in the hypergraph.</p>"},{"location":"api/reference/#hyperbench.types.hypergraph.HIFHypergraph.num_edges","title":"<code>num_edges</code>  <code>property</code>","text":"<p>Return the number of edges in the hypergraph.</p>"},{"location":"api/reference/#hyperbench.types.hypergraph.HIFHypergraph.from_hif","title":"<code>from_hif(data)</code>  <code>classmethod</code>","text":"<p>Create a Hypergraph from a HIF (Hypergraph Interchange Format).</p> <p>Args:     data: Dictionary with keys: network-type, metadata, incidences, nodes, edges</p> <p>Returns:     Hypergraph instance</p> Source code in <code>hyperbench/types/hypergraph.py</code> <pre><code>@classmethod\ndef from_hif(cls, data: Dict[str, Any]) -&gt; \"HIFHypergraph\":\n    \"\"\"\n    Create a Hypergraph from a HIF (Hypergraph Interchange Format).\n\n    Args:\n        data: Dictionary with keys: network-type, metadata, incidences, nodes, edges\n\n    Returns:\n        Hypergraph instance\n    \"\"\"\n    network_type = data.get(\"network-type\") or data.get(\"network_type\")\n    metadata = data.get(\"metadata\", {})\n    incidences = data.get(\"incidences\", [])\n    nodes = data.get(\"nodes\", [])\n    edges = data.get(\"edges\", [])\n\n    return cls(\n        network_type=network_type,\n        metadata=metadata,\n        incidences=incidences,\n        nodes=nodes,\n        edges=edges,\n    )\n</code></pre>"},{"location":"api/reference/#hyperbench.types.hypergraph.Hypergraph","title":"<code>Hypergraph</code>","text":"<p>A simple hypergraph data structure using edge list representation.</p> Source code in <code>hyperbench/types/hypergraph.py</code> <pre><code>class Hypergraph:\n    \"\"\"\n    A simple hypergraph data structure using edge list representation.\n    \"\"\"\n\n    def __init__(self, edges: List[List[int]]):\n        self.edges = edges\n\n    @property\n    def num_nodes(self) -&gt; int:\n        \"\"\"Return the number of nodes in the hypergraph.\"\"\"\n        nodes = set()\n        for edge in self.edges:\n            nodes.update(edge)\n        return len(nodes)\n\n    @property\n    def num_edges(self) -&gt; int:\n        \"\"\"Return the number of edges in the hypergraph.\"\"\"\n        return len(self.edges)\n\n    @classmethod\n    def from_hyperedge_index(cls, hyperedge_index: Tensor) -&gt; \"Hypergraph\":\n        \"\"\"\n        Create a Hypergraph from a hyperedge index representation.\n\n        Args:\n            hyperedge_index: Tensor of shape (2, |E|) representing hyperedges, where each column is (node, hyperedge).\n\n        Returns:\n            Hypergraph instance\n        \"\"\"\n        if hyperedge_index.size(1) &lt; 1:\n            return cls(edges=[])\n\n        max_edge_id = int(hyperedge_index[1].max().item())\n        edges = [\n            hyperedge_index[0, hyperedge_index[1] == edge_id].tolist()\n            for edge_id in range(max_edge_id + 1)\n        ]\n        return cls(edges=edges)\n</code></pre>"},{"location":"api/reference/#hyperbench.types.hypergraph.Hypergraph.num_nodes","title":"<code>num_nodes</code>  <code>property</code>","text":"<p>Return the number of nodes in the hypergraph.</p>"},{"location":"api/reference/#hyperbench.types.hypergraph.Hypergraph.num_edges","title":"<code>num_edges</code>  <code>property</code>","text":"<p>Return the number of edges in the hypergraph.</p>"},{"location":"api/reference/#hyperbench.types.hypergraph.Hypergraph.from_hyperedge_index","title":"<code>from_hyperedge_index(hyperedge_index)</code>  <code>classmethod</code>","text":"<p>Create a Hypergraph from a hyperedge index representation.</p> <p>Args:     hyperedge_index: Tensor of shape (2, |E|) representing hyperedges, where each column is (node, hyperedge).</p> <p>Returns:     Hypergraph instance</p> Source code in <code>hyperbench/types/hypergraph.py</code> <pre><code>@classmethod\ndef from_hyperedge_index(cls, hyperedge_index: Tensor) -&gt; \"Hypergraph\":\n    \"\"\"\n    Create a Hypergraph from a hyperedge index representation.\n\n    Args:\n        hyperedge_index: Tensor of shape (2, |E|) representing hyperedges, where each column is (node, hyperedge).\n\n    Returns:\n        Hypergraph instance\n    \"\"\"\n    if hyperedge_index.size(1) &lt; 1:\n        return cls(edges=[])\n\n    max_edge_id = int(hyperedge_index[1].max().item())\n    edges = [\n        hyperedge_index[0, hyperedge_index[1] == edge_id].tolist()\n        for edge_id in range(max_edge_id + 1)\n    ]\n    return cls(edges=edges)\n</code></pre>"},{"location":"api/reference/#hyperbench.types.hypergraph.HyperedgeIndex","title":"<code>HyperedgeIndex</code>","text":"<p>A wrapper for hyperedge index representation. Hyperedge index is a tensor of shape (2, |E|) that encodes the relationships between nodes and hyperedges. Each column in the tensor represents an incidence between a node and a hyperedge, with the first row containing node indices and the second row containing corresponding hyperedge indices.</p> <p>Example:     hyperedge_index = [[0, 1, 2, 0],                        [0, 0, 0, 1]]</p> <pre><code>This represents two hyperedges:\n    - Hyperedge 0 connects nodes 0, 1, and 2.\n    - Hyperedge 1 connects node 0.\n\nThe number of nodes in this hypergraph is 3 (nodes 0, 1, and 2).\nThe number of hyperedges is 2 (hyperedges 0 and 1).\n</code></pre> Source code in <code>hyperbench/types/hypergraph.py</code> <pre><code>class HyperedgeIndex:\n    \"\"\"\n    A wrapper for hyperedge index representation.\n    Hyperedge index is a tensor of shape (2, |E|) that encodes the relationships between nodes and hyperedges.\n    Each column in the tensor represents an incidence between a node and a hyperedge, with the first row containing node indices\n    and the second row containing corresponding hyperedge indices.\n\n    Example:\n        hyperedge_index = [[0, 1, 2, 0],\n                           [0, 0, 0, 1]]\n\n        This represents two hyperedges:\n            - Hyperedge 0 connects nodes 0, 1, and 2.\n            - Hyperedge 1 connects node 0.\n\n        The number of nodes in this hypergraph is 3 (nodes 0, 1, and 2).\n        The number of hyperedges is 2 (hyperedges 0 and 1).\n    \"\"\"\n\n    def __init__(self, hyperedge_index: Tensor):\n        self.hyperedge_index = hyperedge_index\n\n    @property\n    def item(self) -&gt; Tensor:\n        \"\"\"Return the hyperedge index tensor.\"\"\"\n        return self.hyperedge_index\n\n    @property\n    def num_hyperedges(self) -&gt; int:\n        \"\"\"Return the number of hyperedges in the hypergraph.\"\"\"\n        if self.hyperedge_index.size(1) &lt; 1:\n            return 0\n\n        hyperedges = self.hyperedge_index[1]\n        return int(hyperedges.max().item()) + 1\n\n    @property\n    def num_nodes(self) -&gt; int:\n        \"\"\"Return the number of nodes in the hypergraph.\"\"\"\n        if self.hyperedge_index.size(1) &lt; 1:\n            return 0\n\n        nodes = self.hyperedge_index[0]\n        return int(nodes.max().item()) + 1\n\n    def reduce_to_edge_index_on_random_direction(\n        self,\n        x: Tensor,\n        with_mediators: bool = False,\n        remove_selfloops: bool = True,\n    ) -&gt; Tensor:\n        r\"\"\"\n        Construct a graph from a hypergraph with methods proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs &lt;https://arxiv.org/pdf/1809.02589.pdf&gt;`_ paper\n        Reference implementation: `source &lt;https://deephypergraph.readthedocs.io/en/latest/_modules/dhg/structure/graphs/graph.html#Graph.from_hypergraph_hypergcn&gt;`_.\n\n        Args:\n            x: Node feature matrix. Size ``(|V|, C)``.\n            with_mediator: Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.\n            remove_selfloops: Whether to remove self-loops. Defaults to ``True``.\n\n        Returns:\n            The edge index. Size ``(2, |E'|)``.\n        \"\"\"\n        device = x.device\n\n        hypergraph = Hypergraph.from_hyperedge_index(self.hyperedge_index)\n        hypergraph_edges: List[List[int]] = hypergraph.edges\n        graph_edges: List[List[int]] = []\n\n        # Random direction (feature_dim, 1) for projecting nodes in each hyperedge\n        # Geometrically, we are choosing a random line through the origin in \u211d\u1d48, where \u1d48 = feature_dim\n        random_direction = torch.rand((x.shape[1], 1), device=device)\n\n        for edge in hypergraph_edges:\n            num_nodes_in_edge = len(edge)\n            if num_nodes_in_edge &lt; 2:\n                raise ValueError(\"The number of vertices in an hyperedge must be &gt;= 2.\")\n\n            # projections (num_nodes_in_edge,) contains a scalar value for each node in the hyperedge,\n            # indicating its projection on the random vector 'random_direction'.\n            # Key idea: If two points are very far apart in \u211d\u1d48, there is a high probability\n            # that a random projection will still separate them\n            projections = torch.matmul(x[edge], random_direction).squeeze()\n\n            # The indices of the nodes that the farthest apart in the direction of 'random_direction'\n            node_max_proj_idx = torch.argmax(projections)\n            node_min_proj_idx = torch.argmin(projections)\n\n            if not with_mediators:  # Just connect the two farthest nodes\n                graph_edges.append([edge[node_min_proj_idx], edge[node_max_proj_idx]])\n                continue\n\n            for node_idx in range(num_nodes_in_edge):\n                if node_idx != node_max_proj_idx and node_idx != node_min_proj_idx:\n                    graph_edges.append([edge[node_min_proj_idx], edge[node_idx]])\n                    graph_edges.append([edge[node_max_proj_idx], edge[node_idx]])\n\n        graph = Graph(edges=graph_edges)\n        if remove_selfloops:\n            graph.remove_selfloops()\n\n        return graph.to_edge_index()\n</code></pre>"},{"location":"api/reference/#hyperbench.types.hypergraph.HyperedgeIndex.item","title":"<code>item</code>  <code>property</code>","text":"<p>Return the hyperedge index tensor.</p>"},{"location":"api/reference/#hyperbench.types.hypergraph.HyperedgeIndex.num_hyperedges","title":"<code>num_hyperedges</code>  <code>property</code>","text":"<p>Return the number of hyperedges in the hypergraph.</p>"},{"location":"api/reference/#hyperbench.types.hypergraph.HyperedgeIndex.num_nodes","title":"<code>num_nodes</code>  <code>property</code>","text":"<p>Return the number of nodes in the hypergraph.</p>"},{"location":"api/reference/#hyperbench.types.hypergraph.HyperedgeIndex.reduce_to_edge_index_on_random_direction","title":"<code>reduce_to_edge_index_on_random_direction(x, with_mediators=False, remove_selfloops=True)</code>","text":"<p>Construct a graph from a hypergraph with methods proposed in <code>HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs &lt;https://arxiv.org/pdf/1809.02589.pdf&gt;</code> paper Reference implementation: <code>source &lt;https://deephypergraph.readthedocs.io/en/latest/_modules/dhg/structure/graphs/graph.html#Graph.from_hypergraph_hypergcn&gt;</code>.</p> <p>Args:     x: Node feature matrix. Size <code>(|V|, C)</code>.     with_mediator: Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to <code>False</code>.     remove_selfloops: Whether to remove self-loops. Defaults to <code>True</code>.</p> <p>Returns:     The edge index. Size <code>(2, |E'|)</code>.</p> Source code in <code>hyperbench/types/hypergraph.py</code> <pre><code>def reduce_to_edge_index_on_random_direction(\n    self,\n    x: Tensor,\n    with_mediators: bool = False,\n    remove_selfloops: bool = True,\n) -&gt; Tensor:\n    r\"\"\"\n    Construct a graph from a hypergraph with methods proposed in `HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs &lt;https://arxiv.org/pdf/1809.02589.pdf&gt;`_ paper\n    Reference implementation: `source &lt;https://deephypergraph.readthedocs.io/en/latest/_modules/dhg/structure/graphs/graph.html#Graph.from_hypergraph_hypergcn&gt;`_.\n\n    Args:\n        x: Node feature matrix. Size ``(|V|, C)``.\n        with_mediator: Whether to use mediator to transform the hyperedges to edges in the graph. Defaults to ``False``.\n        remove_selfloops: Whether to remove self-loops. Defaults to ``True``.\n\n    Returns:\n        The edge index. Size ``(2, |E'|)``.\n    \"\"\"\n    device = x.device\n\n    hypergraph = Hypergraph.from_hyperedge_index(self.hyperedge_index)\n    hypergraph_edges: List[List[int]] = hypergraph.edges\n    graph_edges: List[List[int]] = []\n\n    # Random direction (feature_dim, 1) for projecting nodes in each hyperedge\n    # Geometrically, we are choosing a random line through the origin in \u211d\u1d48, where \u1d48 = feature_dim\n    random_direction = torch.rand((x.shape[1], 1), device=device)\n\n    for edge in hypergraph_edges:\n        num_nodes_in_edge = len(edge)\n        if num_nodes_in_edge &lt; 2:\n            raise ValueError(\"The number of vertices in an hyperedge must be &gt;= 2.\")\n\n        # projections (num_nodes_in_edge,) contains a scalar value for each node in the hyperedge,\n        # indicating its projection on the random vector 'random_direction'.\n        # Key idea: If two points are very far apart in \u211d\u1d48, there is a high probability\n        # that a random projection will still separate them\n        projections = torch.matmul(x[edge], random_direction).squeeze()\n\n        # The indices of the nodes that the farthest apart in the direction of 'random_direction'\n        node_max_proj_idx = torch.argmax(projections)\n        node_min_proj_idx = torch.argmin(projections)\n\n        if not with_mediators:  # Just connect the two farthest nodes\n            graph_edges.append([edge[node_min_proj_idx], edge[node_max_proj_idx]])\n            continue\n\n        for node_idx in range(num_nodes_in_edge):\n            if node_idx != node_max_proj_idx and node_idx != node_min_proj_idx:\n                graph_edges.append([edge[node_min_proj_idx], edge[node_idx]])\n                graph_edges.append([edge[node_max_proj_idx], edge[node_idx]])\n\n    graph = Graph(edges=graph_edges)\n    if remove_selfloops:\n        graph.remove_selfloops()\n\n    return graph.to_edge_index()\n</code></pre>"},{"location":"api/reference/#model","title":"Model","text":""},{"location":"api/reference/#hyperbench.types.model","title":"<code>hyperbench.types.model</code>","text":""},{"location":"api/reference/#hyperbench.types.model.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>A class representing the configuration of a model for the MultiModelTrainer trainer.</p> <p>Args:     name: The name of the model.     version: The version of the model.     model: a LightningModule instance.     trainer: a Trainer instance.</p> Source code in <code>hyperbench/types/model.py</code> <pre><code>class ModelConfig:\n    \"\"\"\n    A class representing the configuration of a model for the MultiModelTrainer trainer.\n\n    Args:\n        name: The name of the model.\n        version: The version of the model.\n        model: a LightningModule instance.\n        trainer: a Trainer instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        model: L.LightningModule,\n        version: str = \"default\",\n        trainer: Optional[L.Trainer] = None,\n    ) -&gt; None:\n        self.name = name\n        self.version = version\n        self.model = model\n        self.trainer = trainer\n\n    def full_model_name(self) -&gt; str:\n        return f\"{self.name}:{self.version}\"\n</code></pre>"},{"location":"api/reference/#utils-module","title":"Utils Module","text":""},{"location":"api/reference/#data-utils","title":"Data Utils","text":""},{"location":"api/reference/#hyperbench.utils.data_utils","title":"<code>hyperbench.utils.data_utils</code>","text":""},{"location":"api/reference/#hif-utils","title":"HIF Utils","text":""},{"location":"api/reference/#hyperbench.utils.hif_utils","title":"<code>hyperbench.utils.hif_utils</code>","text":""},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>uv</li> <li>make</li> </ul>"},{"location":"getting-started/installation/#build","title":"Build","text":"<p>To build the project, run: <pre><code>make\n</code></pre></p>"},{"location":"getting-started/installation/#linter-and-type-checker","title":"Linter and type checker","text":"<p>Use Ruff for linting and formatting:</p> <pre><code>make lint\n</code></pre> <p>Use Ty for type checking:</p> <pre><code>make typecheck\n</code></pre> <p>Use the <code>check</code> target to run both linter and type checker:</p> <pre><code>make check\n</code></pre>"},{"location":"getting-started/installation/#tests","title":"Tests","text":"<p>Use pytest to run the test suite:</p> <pre><code>make test\n\n# Run tests with HTML report\nuv run pytest --cov=hyperbench --cov-report=html\n</code></pre>"},{"location":"getting-started/installation/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>Run the following command to install the pre-commit hook:</p> <pre><code>make setup\n\npre-commit install --config .github/hooks/.pre-commit-config.yaml --hook-type pre-commit --install-hooks --overwrite\n</code></pre> <p>This will ensure that your code adheres to the project's coding standards before each commit.</p>"},{"location":"user-guide/user/","title":"User Guide","text":""},{"location":"user-guide/user/#overview","title":"Overview","text":"<p>Hyperbench is designed to simplify working with hypergraph data in machine learning contexts.</p>"}]}