import lightning as L
import logging as log

from collections.abc import Iterable
from lightning.pytorch.accelerators import Accelerator
from lightning.pytorch.callbacks import Callback
from lightning.pytorch.loggers import Logger
from lightning.pytorch.profilers import Profiler
from lightning.pytorch.strategies import Strategy
from pathlib import Path
from typing import Any, Dict, List, Mapping, Optional
from hyperbench.data import DataLoader
from hyperbench.types import CkptStrategy, ModelConfig, TestResult


class MultiModelTrainer:
    r"""
    A trainer class to handle training multiple models with individual trainers.

    Args:
        accelerator: Supports passing different accelerator types ("cpu", "gpu", "tpu", "hpu", "mps", "auto")
            as well as custom accelerator instances.

        devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices
            (list or str), the value ``-1`` to indicate all available devices should be used, or ``"auto"`` for
            automatic selection based on the chosen accelerator. Default: ``"auto"``.

        strategy: Supports different training strategies with aliases as well custom strategies.
            Default: ``"auto"``.

        num_nodes: Number of GPU nodes for distributed training.
            Default: ``1``.

        precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),
            16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').
            Can be used on CPU, GPU, TPUs, or HPUs.
            Default: ``'32-true'``.

        max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).
            If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.
            To enable infinite training, set ``max_epochs = -1``.

        min_epochs: Force training for at least these many epochs. Disabled by default (None).

        max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``
            and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set
            ``max_epochs`` to ``-1``.

        min_steps: Force training for at least these number of steps. Disabled by default (``None``).

        check_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,
            validation will be done solely based on the number of training batches, requiring ``val_check_interval``
            to be an integer value. When used together with a time-based ``val_check_interval`` and
            ``check_val_every_n_epoch`` > 1, validation is aligned to epoch multiples: if the interval elapses
            before the next multiple-N epoch, validation runs at the start of that epoch (after the first batch)
            and the timer resets; if it elapses during a multiple-N epoch, validation runs after the current batch.
            For ``None`` or ``1`` cases, the time-based behavior of ``val_check_interval`` applies without
            additional alignment.
            Default: ``1``.

        logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses
            the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.
            ``False`` will disable logging. If multiple loggers are provided, local files
            (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.
            Default: ``True``.

        default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.
            Default: ``os.getcwd()``.
            Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'

        enable_autolog_hparams: Whether to log hyperparameters at the start of a run.
            Default: ``True``.

        log_every_n_steps: How often to log within steps.
            Default: ``50``.

        profiler: To profile individual steps during training and assist in identifying bottlenecks.
            Default: ``None``.

        fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)
            of train, val and test to find any bugs (ie: a sort of unit test).
            Default: ``False``.

        enable_checkpointing: If ``True``, enable checkpointing.
            It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in
            :paramref:`~hyperbench.train.MultiModelTrainer.callbacks`.
            Default: ``True``.

        enable_progress_bar: Whether to enable the progress bar by default.
            Default: ``True``.

        enable_model_summary: Whether to enable model summarization by default.
            Default: ``True``.

        callbacks: Add a callback or list of callbacks.
            Default: ``None``.
    """

    def __init__(
        self,
        model_configs: List[ModelConfig],
        # args to pass to each Trainer
        accelerator: str | Accelerator = "auto",
        devices: list[int] | str | int = "auto",
        strategy: str | Strategy = "auto",
        num_nodes: int = 1,
        precision: Optional[
            Any  # Any as Lightning accepts multiple types (int, str, Literal, etc.)
        ] = None,
        max_epochs: Optional[int] = None,
        min_epochs: Optional[int] = None,
        max_steps: int = -1,
        min_steps: Optional[int] = None,
        check_val_every_n_epoch: Optional[int] = 1,
        logger: Optional[Logger | Iterable[Logger] | bool] = None,
        default_root_dir: Optional[str | Path] = None,
        enable_autolog_hparams: bool = True,
        log_every_n_steps: Optional[int] = None,
        profiler: Optional[Profiler | str] = None,
        fast_dev_run: int | bool = False,
        enable_checkpointing: bool = True,
        enable_progress_bar: bool = True,
        enable_model_summary: Optional[bool] = None,
        callbacks: Optional[List[Callback] | Callback] = None,
        **kwargs,
    ) -> None:
        self.model_configs: List[ModelConfig] = model_configs

        for model_config in model_configs:
            if model_config.trainer is None:
                model_config.trainer = L.Trainer(
                    accelerator=accelerator,
                    devices=devices,
                    strategy=strategy,
                    num_nodes=num_nodes,
                    precision=precision,
                    max_epochs=max_epochs,
                    min_epochs=min_epochs,
                    max_steps=max_steps,
                    min_steps=min_steps,
                    check_val_every_n_epoch=check_val_every_n_epoch,
                    logger=logger,
                    default_root_dir=default_root_dir,
                    enable_autolog_hparams=enable_autolog_hparams,
                    log_every_n_steps=log_every_n_steps,
                    profiler=profiler,
                    fast_dev_run=fast_dev_run,
                    enable_checkpointing=enable_checkpointing,
                    enable_progress_bar=enable_progress_bar,
                    enable_model_summary=enable_model_summary,
                    callbacks=callbacks,
                    **kwargs,
                )

    @property
    def models(self) -> List[L.LightningModule]:
        return [config.model for config in self.model_configs]

    def model(self, name: str, version: str = "default") -> Optional[L.LightningModule]:
        for config in self.model_configs:
            if config.name == name and config.version == version:
                return config.model
        return None

    def fit_all(
        self,
        train_dataloader: Optional[DataLoader] = None,
        val_dataloader: Optional[DataLoader] = None,
        datamodule: Optional[L.LightningDataModule] = None,
        ckpt_path: Optional[CkptStrategy] = None,
        verbose: bool = True,
    ) -> None:
        if len(self.model_configs) < 1:
            raise ValueError("No models to fit.")

        for i, config in enumerate(self.model_configs):
            if config.trainer is None:
                raise ValueError(f"Trainer not defined for model {config.full_model_name()}.")

            if verbose:
                log.info(
                    f"Fit model {config.full_model_name()} [{i + 1}/{len(self.model_configs)}]\n"
                )

            config.trainer.fit(
                model=config.model,
                train_dataloaders=train_dataloader,
                val_dataloaders=val_dataloader,
                datamodule=datamodule,
                ckpt_path=ckpt_path,
            )

    def test_all(
        self,
        dataloader: Optional[DataLoader] = None,
        datamodule: Optional[L.LightningDataModule] = None,
        ckpt_path: Optional[CkptStrategy] = None,
        verbose: bool = True,
        verbose_loop: bool = True,
    ) -> Mapping[str, TestResult]:
        if len(self.model_configs) < 1:
            raise ValueError("No models to test.")

        test_results: Dict[str, TestResult] = {}

        for i, config in enumerate(self.model_configs):
            if config.trainer is None:
                raise ValueError(f"Trainer not defined for model {config.full_model_name()}.")

            if verbose:
                log.info(
                    f"Test model {config.full_model_name()} [{i + 1}/{len(self.model_configs)}]\n"
                )

            trainer_test_results: List[TestResult] = config.trainer.test(
                model=config.model,
                dataloaders=dataloader,
                datamodule=datamodule,
                ckpt_path=ckpt_path,
                verbose=verbose_loop,
            )

            # In Lightning, test() returns a list of dicts, one per dataloader, but we use a single dataloader
            test_results[config.full_model_name()] = (
                trainer_test_results[0] if len(trainer_test_results) > 0 else {}
            )

        return test_results
